# -*- coding: utf-8 -*-
"""hybrid architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sWYXCXtRR4t89Qgjdu5qplsPKc6zRn_s

CNN–Transformer hybrid architecture
"""

# ==========================================================
# CNN–Transformer Hybrid Model (Conv1D + Transformer Encoder)
# Works with pooled BERT embeddings shaped (N, D)
# -> reshape to pseudo-sequence (N, seq_len, d_model)
# -> CNN feature extractor over "time"
# -> Transformer encoder blocks (Multi-Head Self-Attention)
# -> Global pooling + Dense multi-label head (5 traits)
# ==========================================================
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
import time
import tensorflow as tf
from keras import Model, Input
from keras.layers import (
    Dense, Dropout, LayerNormalization, MultiHeadAttention,
    GlobalAveragePooling1D, Conv1D
)
from keras.optimizers import Adam

# ----------------------------
# Reproducibility
# ----------------------------
np.random.seed(42)
tf.random.set_seed(42)

# ----------------------------
# Load and preprocess data
# ----------------------------
data_path = "/kaggle/input/personality-datasets1/essays_preprocessing.csv"
emb_path  = "/kaggle/input/personality-datasets1/bert_embeddings_essays.pkl"

data = pd.read_csv(data_path)
data.dropna(inplace=True)
print("Data shape:", data.shape)

# ----------------------------
# Load embeddings
# ----------------------------
embeddings = joblib.load(emb_path)
X = np.asarray(embeddings, dtype=np.float32)
print("Embeddings shape:", X.shape)  # (N, D)

# ----------------------------
# Labels (multi-label, 5 traits)
# ----------------------------
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
y = data[traits].to_numpy().astype(np.float32)
print("Labels shape:", y.shape)
if X.shape[0] != y.shape[0]:
    raise ValueError(f"Mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows")

# ----------------------------
# Split data
# ----------------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.333, random_state=42
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

# ----------------------------
# Convert pooled embedding (N, D) -> pseudo-sequence (N, seq_len, d_model)
# ----------------------------
D = X_train.shape[1]

preferred_seq_lens = [24, 16, 12, 8, 6, 4, 3, 2]
seq_len = None
for s in preferred_seq_lens:
    if D % s == 0:
        seq_len = s
        break
if seq_len is None:
    seq_len = 1

d_model = D // seq_len
print(f"Using seq_len={seq_len}, d_model={d_model} (since D={D})")

def to_sequence(X_in: np.ndarray) -> np.ndarray:
    return X_in.reshape(X_in.shape[0], seq_len, d_model)

X_train_seq = to_sequence(X_train)
X_val_seq   = to_sequence(X_val)
X_test_seq  = to_sequence(X_test)

print("Train seq:", X_train_seq.shape)

# ----------------------------
# Transformer encoder block
# ----------------------------
def transformer_encoder_block(x, num_heads=4, ff_dim=256, dropout=0.2):
    # Self-attention
    attn_out = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=int(x.shape[-1])
    )(x, x)
    attn_out = Dropout(dropout)(attn_out)
    x = LayerNormalization(epsilon=1e-6)(x + attn_out)

    # Feed-forward
    ff = Dense(ff_dim, activation="relu")(x)
    ff = Dropout(dropout)(ff)
    ff = Dense(int(x.shape[-1]))(ff)
    x = LayerNormalization(epsilon=1e-6)(x + ff)
    return x

# ----------------------------
# Build CNN–Transformer Hybrid model
# ----------------------------
inputs = Input(shape=(seq_len, d_model), name="embedding_sequence")

# CNN front-end (local pattern extractor over pseudo-time)
x = Conv1D(filters=128, kernel_size=3, padding="same", activation="relu")(inputs)
x = Dropout(0.2)(x)
x = Conv1D(filters=128, kernel_size=5, padding="same", activation="relu")(x)
x = Dropout(0.2)(x)

# Project to a stable dimension for attention (optional but often helpful)
x = Dense(128)(x)  # now features are 128

# Transformer encoder stack
x = transformer_encoder_block(x, num_heads=4, ff_dim=256, dropout=0.2)
x = transformer_encoder_block(x, num_heads=4, ff_dim=256, dropout=0.2)

# Pool + classifier
x = GlobalAveragePooling1D()(x)
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)
outputs = Dense(len(traits), activation="sigmoid", name="trait_outputs")(x)

model = Model(inputs, outputs, name="cnn_transformer_hybrid_personality")

model.compile(
    optimizer=Adam(learning_rate=2e-4),
    loss="binary_crossentropy",
    metrics=[
        tf.keras.metrics.AUC(name="auc"),
        tf.keras.metrics.BinaryAccuracy(name="bin_acc", threshold=0.5),
    ]
)
model.summary()

# ----------------------------
# Callbacks
# ----------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        min_lr=1e-5
    )
]

# ----------------------------
# Train
# ----------------------------
history = model.fit(
    X_train_seq, y_train,
    validation_data=(X_val_seq, y_val),
    epochs=15,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)
# ----------------------------
# Evaluate
# ----------------------------
y_prob = model.predict(X_test_seq)
y_pred = (y_prob >= 0.5).astype(int)
macro_f1 = f1_score(y_test, y_pred, average="macro")
micro_f1 = f1_score(y_test, y_pred, average="micro")
print("\nTest Macro F1:", macro_f1)
print("Test Micro F1:", micro_f1)

print("\nPer-trait classification report:")
print(classification_report(y_test, y_pred, target_names=traits, zero_division=0))