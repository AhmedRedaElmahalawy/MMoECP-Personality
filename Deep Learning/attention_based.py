# -*- coding: utf-8 -*-
"""Attention-based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sWYXCXtRR4t89Qgjdu5qplsPKc6zRn_s

Attention-based (BiLSTM + Attention) model
"""

# ============================================
# BiLSTM + Attention (Attention-based Model)
# Works with pooled BERT embeddings shaped (N, D)
# Converts embeddings into a pseudo-sequence, then applies BiLSTM + Attention
# ============================================
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
import tensorflow as tf
from keras import Model, Input
from keras.layers import Dense, Dropout, Bidirectional, LSTM, Layer, GlobalAveragePooling1D
from keras.optimizers import Adam
# ----------------------------
# Reproducibility
# ----------------------------
np.random.seed(42)
tf.random.set_seed(42)

# ----------------------------
# Load and preprocess data
# ----------------------------
data_path = "/kaggle/input/personality-datasets1/essays_preprocessing.csv"
emb_path  = "/kaggle/input/personality-datasets1/bert_embeddings_essays.pkl"
data = pd.read_csv(data_path)
data.dropna(inplace=True)
print("Data shape:", data.shape)

# ----------------------------
# Load embeddings
# ----------------------------
embeddings = joblib.load(emb_path)
X = np.asarray(embeddings, dtype=np.float32)
print("Embeddings shape:", X.shape)  # (N, D)

# ----------------------------
# Labels (multi-label, 5 traits)
# ----------------------------
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
y = data[traits].to_numpy().astype(np.float32)
print("Labels shape:", y.shape)
if X.shape[0] != y.shape[0]:
    raise ValueError(f"Mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows")

# ----------------------------
# Train/Val/Test Split
# ----------------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.333, random_state=42
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

# ----------------------------
# Convert pooled embedding (N, D) to pseudo-sequence (N, seq_len, d_model)
# so the BiLSTM and Attention have a "time" dimension.
# ----------------------------
D = X_train.shape[1]
preferred_seq_lens = [16, 12, 8, 6, 4, 3, 2]
seq_len = None
for s in preferred_seq_lens:
    if D % s == 0:
        seq_len = s
        break
if seq_len is None:
    seq_len = 1

d_model = D // seq_len
print(f"Using seq_len={seq_len}, d_model={d_model} (since D={D})")

def to_sequence(X_in: np.ndarray) -> np.ndarray:
    return X_in.reshape(X_in.shape[0], seq_len, d_model)

X_train_seq = to_sequence(X_train)
X_val_seq   = to_sequence(X_val)
X_test_seq  = to_sequence(X_test)

print("Train seq:", X_train_seq.shape)

# ----------------------------
# Simple Attention Layer (Bahdanau-style scoring over time)
# ----------------------------
class TemporalAttention(Layer):
    """
    Learns attention weights over the time dimension.
    Input:  (batch, time, features)
    Output: (batch, features) context vector
    """
    def __init__(self, attn_units=128, **kwargs):
        super().__init__(**kwargs)
        self.attn_units = attn_units
        self.W = Dense(attn_units, activation="tanh")
        self.v = Dense(1, use_bias=False)

    def call(self, x):
        # x: (B, T, F)
        score = self.v(self.W(x))          # (B, T, 1)
        weights = tf.nn.softmax(score, axis=1)  # (B, T, 1)
        context = tf.reduce_sum(weights * x, axis=1)  # (B, F)
        return context

# ----------------------------
# Build BiLSTM + Attention model
# ----------------------------
inputs = Input(shape=(seq_len, d_model), name="bert_embedding_sequence")

x = Bidirectional(LSTM(64, return_sequences=True))(inputs)
x = Dropout(0.3)(x)

x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = Dropout(0.3)(x)

context = TemporalAttention(attn_units=64, name="temporal_attention")(x)

x = Dense(128, activation="relu")(context)
x = Dropout(0.3)(x)

outputs = Dense(len(traits), activation="sigmoid", name="trait_outputs")(x)

model = Model(inputs, outputs, name="bilstm_attention_personality_model")

model.compile(
    optimizer=Adam(learning_rate=2e-4),
    loss="binary_crossentropy",
    metrics=[
        tf.keras.metrics.AUC(name="auc"),
        tf.keras.metrics.BinaryAccuracy(name="bin_acc", threshold=0.5),
    ]
)
model.summary()

# ----------------------------
# Callbacks
# ----------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        min_lr=1e-6
    )
]

# ----------------------------
# Train
# ----------------------------
history = model.fit(
    X_train_seq, y_train,
    validation_data=(X_val_seq, y_val),
    epochs=30,
    batch_size=64,
    callbacks=callbacks,
    verbose=1
)
# ----------------------------
# Evaluate
# ----------------------------
y_prob = model.predict(X_test_seq)
y_pred = (y_prob >= 0.5).astype(int)
macro_f1 = f1_score(y_test, y_pred, average="macro")
micro_f1 = f1_score(y_test, y_pred, average="micro")
print("\nTest Macro F1:", macro_f1)
print("Test Micro F1:", micro_f1)

print("\nPer-trait classification report:")
print(classification_report(y_test, y_pred, target_names=traits, zero_division=0))