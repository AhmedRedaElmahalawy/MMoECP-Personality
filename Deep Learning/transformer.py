# -*- coding: utf-8 -*-
"""Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sWYXCXtRR4t89Qgjdu5qplsPKc6zRn_s
"""

# ============================================
# Full Attention-based (Transformer) Model Code
# Works with pooled BERT embeddings shaped (N, D)
# Converts embeddings into a pseudo-sequence for attention
# ============================================
import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
import tensorflow as tf
from keras import Model, Input
from keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.optimizers import Adam
# ----------------------------
# Reproducibility
# ----------------------------
np.random.seed(42)
tf.random.set_seed(42)
# ----------------------------
# Load and preprocess data
# ----------------------------
data_path = "/kaggle/input/personality-datasets1/essays_preprocessing.csv"
emb_path  = "/kaggle/input/personality-datasets1/bert_embeddings_essays.pkl"
data = pd.read_csv(data_path)
data.dropna(inplace=True)
print("Data shape:", data.shape)

# ----------------------------
# Load embeddings
# ----------------------------
embeddings = joblib.load(emb_path)
# embeddings may be list -> convert safely
X = np.asarray(embeddings, dtype=np.float32)
print("Embeddings shape:", X.shape)  # (N, D)

# ----------------------------
# Labels (multi-label, 5 traits)
# ----------------------------
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
labels = data[traits]
y = labels.to_numpy().astype(np.float32)
print("Labels shape:", y.shape)
# Optional sanity check: ensure same N
if X.shape[0] != y.shape[0]:
    raise ValueError(f"Mismatch: X has {X.shape[0]} rows but y has {y.shape[0]} rows")

# ----------------------------
# Train/Val/Test Split
# ----------------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.333, random_state=42
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

# ----------------------------
# Convert pooled embedding (N, D) to pseudo-sequence (N, seq_len, d_model)
# Attention needs a sequence.
# ----------------------------
D = X_train.shape[1]
# Choose a seq_len that divides D nicely.
# For common BERT pooled embeddings D=768 -> seq_len=12 -> d_model=64 works well.
# If D is different, we find a reasonable seq_len automatically.
preferred_seq_lens = [16, 12, 8, 6, 4, 3, 2]
seq_len = None
for s in preferred_seq_lens:
    if D % s == 0:
        seq_len = s
        break

if seq_len is None:
    # fallback: seq_len = 1 (no real attention benefit but code still runs)
    seq_len = 1

d_model = D // seq_len
print(f"Using seq_len={seq_len}, d_model={d_model} (since D={D})")

def to_sequence(X_in: np.ndarray) -> np.ndarray:
    return X_in.reshape(X_in.shape[0], seq_len, d_model)

X_train_seq = to_sequence(X_train)
X_val_seq   = to_sequence(X_val)
X_test_seq  = to_sequence(X_test)
print("Train seq:", X_train_seq.shape)
# ----------------------------
# Transformer block
# ----------------------------
def transformer_block(x, num_heads=4, ff_dim=128, dropout=0.2):
    # Self-attention
    attn_out = MultiHeadAttention(
        num_heads=num_heads,
        key_dim=int(x.shape[-1])
    )(x, x)
    attn_out = Dropout(dropout)(attn_out)
    x = LayerNormalization(epsilon=1e-6)(x + attn_out)

    # Feed-forward network
    ff_out = Dense(ff_dim, activation="relu")(x)
    ff_out = Dropout(dropout)(ff_out)
    ff_out = Dense(int(x.shape[-1]))(ff_out)
    x = LayerNormalization(epsilon=1e-6)(x + ff_out)
    return x

# ----------------------------
# Build Attention-based model
# ----------------------------
inputs = Input(shape=(seq_len, d_model), name="bert_embedding_sequence")

x = transformer_block(inputs, num_heads=4, ff_dim=128, dropout=0.2)
x = transformer_block(x, num_heads=4, ff_dim=128, dropout=0.2)

x = GlobalAveragePooling1D()(x)
x = Dense(128, activation="relu")(x)
x = Dropout(0.3)(x)

outputs = Dense(len(traits), activation="sigmoid", name="trait_outputs")(x)
model = Model(inputs, outputs, name="attention_personality_model")
model.compile(
    optimizer=Adam(learning_rate=2e-4),
    loss="binary_crossentropy",
    metrics=[
        tf.keras.metrics.AUC(name="auc"),
        tf.keras.metrics.BinaryAccuracy(name="bin_acc", threshold=0.5),
    ]
)
model.summary()

# ----------------------------
# Callbacks
# ----------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        min_lr=1e-6
    )
]

# ----------------------------
# Train
# ----------------------------
history = model.fit(
    X_train_seq, y_train,
    validation_data=(X_val_seq, y_val),
    epochs=30,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

# ----------------------------
# Evaluate (F1, per-trait report)
# ----------------------------
y_prob = model.predict(X_test_seq)
y_pred = (y_prob >= 0.5).astype(int)

macro_f1 = f1_score(y_test, y_pred, average="macro")
micro_f1 = f1_score(y_test, y_pred, average="micro")
print("\nTest Macro F1:", macro_f1)
print("Test Micro F1:", micro_f1)
print("\nPer-trait classification report:")
print(classification_report(y_test, y_pred, target_names=traits, zero_division=0))
y_pred_best = (y_prob >= 0.5).astype(int)
macro_f1_best = f1_score(y_test, y_pred_best, average="macro")
micro_f1_best = f1_score(y_test, y_pred_best, average="micro")

print("\nTest Macro F1 :", macro_f1_best)
print("Test Micro F1 :", micro_f1_best)