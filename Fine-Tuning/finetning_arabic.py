# -*- coding: utf-8 -*-
"""finetning_arabic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-m1_opUy-V20_fCg7u-ng12zcvgU-m4a
"""

import torch
from transformers import BertTokenizer, BertModel
import numpy as np
import pandas as pd
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
print(data.dropna(inplace=True))
print("Data shape:", data.shape)
data.head()

# Load and preprocess your Arabic text data
texts = data['text'][0:92]  # List of texts (in Arabic)
# labels = data['NS'].values[0:3000]  # List of corresponding labels (optional)

# Load pre-trained AraBERT model and tokenizer
model_name = 'aubmindlab/bert-base-arabertv02'  # Specify the AraBERT model
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define a function to tokenize and encode the Arabic text data
def get_bert_embeddings(texts):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_text = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids.append(encoded_text['input_ids'])
        attention_masks.append(encoded_text['attention_mask'])

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)

    with torch.no_grad():
        embeddings = model(input_ids.to(device), attention_mask=attention_masks.to(device))

    return embeddings[0][:, 0, :].cpu().numpy()

# Split the data into smaller batches for processing
batch_size = 32
num_batches = (len(texts) - 1) // batch_size + 1

embeddings = []
for i in range(num_batches):
    start = i * batch_size
    end = min((i + 1) * batch_size, len(texts))  # Adjust the end index for the last batch
    batch_texts = texts[start:end]

    try:
        batch_embeddings = get_bert_embeddings(batch_texts)
        embeddings.append(batch_embeddings)
    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            # Handle the out-of-memory error gracefully
            print("CUDA out of memory error occurred. Skipping batch {}.".format(i))
            # Additional error handling or memory optimization can be performed here
        else:
            # Handle other runtime errors
            print("Runtime error occurred:", e)
            # Additional error handling can be performed here

embeddings = np.concatenate(embeddings, axis=0)

# Save the BERT embeddings
joblib.dump(embeddings, 'bert_embeddings_essays_arabic1.pkl')