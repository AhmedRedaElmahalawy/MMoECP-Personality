# -*- coding: utf-8 -*-
"""AraPersonality-MTL-Upload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_lsMHi7mYiRBoJmGljoA4DYaNRlW4duj

CNN
"""

import time
import numpy as np
import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve

import tensorflow as tf
from keras import Sequential, Model, regularizers
from keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.losses import Loss
from keras.metrics import BinaryAccuracy, AUC

# -----------------------------
# Reproducibility
# -----------------------------
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# -----------------------------
# Load data
# -----------------------------
data = pd.read_csv('/kaggle/input/personality-datasets1/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

traits = ['E', 'N', 'A', 'C', 'O']
y_all = data[traits].to_numpy(dtype=np.float32)

embeddings = joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_512_128_arabic.pkl')
X_all = np.array(embeddings, dtype=np.float32)

num_tasks = y_all.shape[1]

# -----------------------------
# ONE split: 70/20/10
# -----------------------------
# 1) Split off TEST = 10%
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_all, y_all,
    test_size=0.10,
    random_state=GLOBAL_SEED,
    shuffle=True
)

# 2) VAL = 20% of full => 20/90 = 2/9 of train_val
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=(2/9),
    random_state=GLOBAL_SEED,
    shuffle=True
)

print(f"Full:  {len(X_all)}")
print(f"Train: {len(X_train)} ({len(X_train)/len(X_all)*100:.2f}%)")
print(f"Val:   {len(X_val)} ({len(X_val)/len(X_all)*100:.2f}%)")
print(f"Test:  {len(X_test)} ({len(X_test)/len(X_all)*100:.2f}%)")

# -----------------------------
# Threshold tuning (per label)
# -----------------------------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

# -----------------------------
# Weighted BCE (computed once from TRAIN)
# -----------------------------
class MultiTaskWeightedBCE(Loss):
    """
    Weighted BCE on probabilities (NOT logits).
    pos_weight_vec shape: (num_tasks,)
    """
    def __init__(self, pos_weight_vec):
        super().__init__()
        self.pos_weight = tf.reshape(tf.cast(pos_weight_vec, tf.float32), (1, -1))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)

        pos_term = - self.pos_weight * y_true * tf.math.log(y_pred)
        neg_term = - (1. - y_true) * tf.math.log(1. - y_pred)

        # sum over tasks, mean over batch
        return tf.reduce_mean(tf.reduce_sum(pos_term + neg_term, axis=1))

# -----------------------------
# CNN block
# -----------------------------
class CNN(Sequential):
    def __init__(self,
                 num_conv_layers: int,
                 filters: int,
                 kernel_size: int,
                 pool_size: int,
                 dense_units: int,
                 dim_out: int = None,
                 dropout: float = 0.0,
                 l2: float = 1e-4,
                 name: str = "CNN"):
        layers = []
        layers.append(Reshape((-1, 1)))  # (bs, D) -> (bs, D, 1)

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters,
                                 kernel_size=kernel_size,
                                 activation='relu',
                                 kernel_regularizer=regularizers.l2(l2)))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu',
                                kernel_regularizer=regularizers.l2(l2)))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))  # logits

        super().__init__(layers, name=name)

# -----------------------------
# MG-MoE with CNN experts/towers
# -----------------------------
class MultiGateMixtureOfExperts(Model):
    def __init__(self,
                 num_tasks: int,
                 num_emb: int,
                 num_experts: int = 3,

                 num_conv_layers_expert: int = 3,
                 filters_expert: int = 64,
                 kernel_size_expert: int = 3,
                 pool_size_expert: int = 2,
                 dense_units_expert: int = 64,
                 dropout_expert: float = 0.1,
                 l2_expert: float = 1e-2,

                 gate_function: str = "softmax",

                 num_conv_layers_tasks: int = 2,
                 filters_tasks: int = 64,
                 kernel_size_tasks: int = 3,
                 pool_size_tasks: int = 2,
                 dense_units_tasks: int = 64,
                 dim_out_tasks: int = 1,
                 dropout_tasks: float = 0.1,
                 l2_tasks: float = 1e-2):
        super().__init__()

        self.experts = [
            CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert,
                dense_units_expert, dropout=dropout_expert, l2=l2_expert, name=f"expert_{i}")
            for i in range(num_experts)
        ]

        self.towers = [
            CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks,
                dense_units_tasks, dim_out_tasks, dropout=dropout_tasks, l2=l2_tasks, name=f"tower_{t}")
            for t in range(num_tasks)
        ]

        self.gates = [
            Dense(num_experts, activation=gate_function, use_bias=False, name=f"gate_{t}")
            for t in range(num_tasks)
        ]

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts: (bs, D) -> (bs, H) for each expert
        out_experts = []
        expert_input = tf.expand_dims(inputs, -1)  # (bs, D, 1)
        for expert in self.experts:
            out_experts.append(expert(expert_input, training=training))  # (bs, H)

        out_experts = tf.stack(out_experts, axis=-1)  # (bs, H, E)

        # Tasks
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)              # (bs, E)
            mixed = tf.einsum("bhe,be->bh", out_experts, gate_score)  # (bs, H)
            logits = tower(tf.expand_dims(mixed, -1), training=training)  # (bs, 1)
            out_tasks.append(logits)

        out = tf.concat(out_tasks, axis=-1)  # (bs, num_tasks) logits
        return tf.nn.sigmoid(out)            # probabilities

# -----------------------------
# Training (ONE run)
# -----------------------------
epochs = 30
batch_size = 64

# pos_weight from TRAIN only
eps = 1e-8
pos_counts = np.sum(y_train, axis=0)
neg_counts = y_train.shape[0] - pos_counts
pos_weight = (neg_counts + eps) / (pos_counts + eps)   # (num_tasks,)
pos_weight_tf = tf.constant(pos_weight, dtype=tf.float32)

print("pos_weight (from TRAIN):", dict(zip(traits, np.round(pos_weight, 4))))

# Fresh model
tf.keras.backend.clear_session()
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

num_embeddings = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    num_experts=3,

    num_conv_layers_expert=3, filters_expert=64, kernel_size_expert=3, pool_size_expert=2,
    dense_units_expert=64, dropout_expert=0.1, l2_expert=1e-4,

    gate_function="softmax",

    num_conv_layers_tasks=2, filters_tasks=64, kernel_size_tasks=3, pool_size_tasks=2,
    dense_units_tasks=64, dim_out_tasks=1, dropout_tasks=0.1, l2_tasks=1e-4
)

loss = MultiTaskWeightedBCE(pos_weight_tf)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=[
        BinaryAccuracy(name="bin_acc"),
        AUC(name="auc", multi_label=True)
    ]
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_auc", mode="max", patience=7, restore_best_weights=True
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_auc", mode="max", factor=0.6, patience=3, min_lr=1e-6, verbose=1
)

start = time.process_time()

model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop, reduce_lr],
    verbose=2
)

elapsed_cpu = time.process_time() - start

# -----------------------------
# Threshold tuning on VAL
# -----------------------------
y_val_prob = model.predict(X_val, verbose=0)
thr = best_thresholds(y_val, y_val_prob)
# -----------------------------
# Evaluate on TEST
# -----------------------------
y_test_prob = model.predict(X_test, verbose=0)
y_pred_labels = (y_test_prob >= thr).astype(int)

f1_micro = f1_score(y_test, y_pred_labels, average='micro', zero_division=0) * 100
f1_macro = f1_score(y_test, y_pred_labels, average='macro', zero_division=0) * 100

print("\n================ FINAL TEST RESULTS ================")
print(f"F1-micro: {f1_micro:.2f}")
print(f"F1-macro: {f1_macro:.2f}")

for i, name in enumerate(traits):
    f1_i = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) * 100
    print(f"  Task {name} - F1: {f1_i:.2f}")

print(f"\nCPU time (train only): {elapsed_cpu:.2f} s")

"""MLP"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score
import tensorflow as tf

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['E', 'N', 'A', 'C', 'O']

# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Define labels for each task
y1 = labels['E']
y2 = labels['N']
y3 = labels['A']
y4 = labels['C']
y5 = labels['O']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# MLP Experts with updated parameters
class MLPExpert:
    def __init__(self, hidden_layer_sizes=(200,), max_iter=200, activation='logistic', solver='adam', alpha=0.0002, learning_rate_init=0.1, learning_rate='constant'):
        # Create an MLP classifier for each expert with the specified hyperparameters
        self.model = MLPClassifier(
            random_state=42,
            hidden_layer_sizes=hidden_layer_sizes,
            max_iter=max_iter,
            activation=activation,
            solver=solver,
            alpha=alpha,
            learning_rate_init=learning_rate_init,
            learning_rate=learning_rate
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with MLP Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (MLP models) and gates for each task
        self.experts = [MLPExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="relu")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)


# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""BERT"""

import time
import numpy as np
import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve

import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy

# -----------------------------
# Reproducibility
# -----------------------------
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# -----------------------------
# Load data
# -----------------------------
data = pd.read_csv('/kaggle/input/personality-datasets1/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

embeddings = joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings, dtype=np.float32)

traits = ['E', 'N', 'A', 'C', 'O']
y = data[traits].to_numpy(dtype=np.float32)

num_tasks = y.shape[1]

# -----------------------------
# ONE split: 70/20/10
# -----------------------------
# 1) Split off TEST = 10%
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y,
    test_size=0.10,
    random_state=GLOBAL_SEED,
    shuffle=True
)

# 2) VAL = 20% of full => 20/90 = 2/9 of train_val
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=(2/9),
    random_state=GLOBAL_SEED,
    shuffle=True
)

print(f"Full:  {len(X)}")
print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.2f}%)")
print(f"Val:   {len(X_val)} ({len(X_val)/len(X)*100:.2f}%)")
print(f"Test:  {len(X_test)} ({len(X_test)/len(X)*100:.2f}%)")

# -----------------------------
# Loss Function
# -----------------------------
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=None) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)           # (batch, tasks)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        return tf.reduce_sum(loss)

# -----------------------------
# Model blocks
# -----------------------------
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super().__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # (batch, seq=1, embed_dim)
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3,
                 num_heads: int = 8, ff_dim: int = 64, dropout_rate: float = 0.01):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate)
                        for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False)
                      for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch, num_experts, hidden_dim)

        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)                       # (batch, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)
            output = tower(mixed_expert, training=training)  # (batch, 1)
            task_outputs.append(output)

        out = tf.concat(task_outputs, axis=-1)              # (batch, num_tasks)
        return tf.nn.sigmoid(out)

# -----------------------------
# Per-label threshold tuning
# -----------------------------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

# -----------------------------
# Train once (ONE run)
# -----------------------------
num_tasks = 5
epochs = 5
batch_size = 32

tf.keras.backend.clear_session()
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

embed_dim = X_train.shape[1]
model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.01
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

start = time.process_time()

model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    verbose=1
)

elapsed_cpu = time.process_time() - start

# -----------------------------
# Threshold tuning on VAL
# -----------------------------
y_val_prob = model.predict(X_val, verbose=0)
thr = best_thresholds(y_val, y_val_prob)
# -----------------------------
# Evaluate on TEST
# -----------------------------
y_test_prob = model.predict(X_test, verbose=0)
y_pred_labels = (y_test_prob >= 0.5).astype(int)

f1_micro = f1_score(y_test, y_pred_labels, average='micro', zero_division=0) * 100
f1_macro = f1_score(y_test, y_pred_labels, average='macro', zero_division=0) * 100

print("\n================ FINAL TEST RESULTS ================")
print(f"F1-micro: {f1_micro:.2f}")
print(f"F1-macro: {f1_macro:.2f}")

for i in range(num_tasks):
    f1_task = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) * 100
    print(f"  Task {traits[i]} - F1: {f1_task:.2f}")

print(f"\nCPU time (train only): {elapsed_cpu:.2f} s")

# Optional: save outputs
# joblib.dump(y_pred_labels, 'y_pred_labels_single_run.pkl')
# joblib.dump(thr, 'val_tuned_thresholds_single_run.pkl')

"""LR"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
import tensorflow as tf

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['E', 'N', 'A', 'C', 'O']

# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Define labels for each task
y1 = labels['E']
y2 = labels['N']
y3 = labels['A']
y4 = labels['C']
y5 = labels['O']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Logistic Regression Experts
class LogisticRegressionExpert:
    def __init__(self, C=0.03, max_iter=500):
        # Create a logistic regression classifier for each expert with optimized hyperparameters
        self.model = LogisticRegression(
            random_state=42,
            max_iter=max_iter,
            C=C, solver='liblinear'
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Logistic Regression Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (LogisticRegression models) and gates for each task
        self.experts = [LogisticRegressionExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 2

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)


# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""XGB"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve
import tensorflow as tf
from xgboost import XGBClassifier
import time

# -----------------------------
# Reproducibility
# -----------------------------
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# -----------------------------
# Load data
# -----------------------------
data = pd.read_csv('/kaggle/input/personality-datasets1/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

traits = ['E', 'N', 'A', 'C', 'O']
labels = data[traits]

embeddings = joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_512_128_arabic.pkl')
X_all = np.array(embeddings, dtype=np.float32)

y_all = np.column_stack([
    labels['E'], labels['N'], labels['A'], labels['C'], labels['O']
]).astype(np.int32)

# Replace -1 with 0 (if exists)
y_all = np.where(y_all == -1, 0, y_all)

num_tasks = y_all.shape[1]
labels_name = traits

# -----------------------------
# ONE split: 70/20/10 (no folds)
# -----------------------------
# 1) TEST = 10%
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_all, y_all,
    test_size=0.10,
    random_state=GLOBAL_SEED,
    shuffle=True
)

# 2) VAL = 20% of full => 20/90 = 2/9 of train_val
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=(2/9),
    random_state=GLOBAL_SEED,
    shuffle=True
)

print(f"Full:  {len(X_all)}")
print(f"Train: {len(X_train)} ({len(X_train)/len(X_all)*100:.2f}%)")
print(f"Val:   {len(X_val)} ({len(X_val)/len(X_all)*100:.2f}%)")
print(f"Test:  {len(X_test)} ({len(X_test)/len(X_all)*100:.2f}%)")

# -----------------------------
# Per-label threshold tuning (VAL)
# -----------------------------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        yt = y_true[:, i].astype(int)
        yp = y_prob[:, i].astype(float)

        # If only one class in validation, PR curve undefined
        if len(np.unique(yt)) < 2:
            thresholds[i] = 0.5
            continue

        p, r, th = precision_recall_curve(yt, yp)
        if th.size == 0:
            thresholds[i] = 0.5
            continue

        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

# ----------------------------
# XGBClassifier Expert
# ----------------------------
class XGBExpert:
    def __init__(self, seed=42):
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=2,
            learning_rate=0.1,
            objective='binary:logistic',
            eval_metric='logloss',
            random_state=seed,
            n_jobs=-1
        )

    def train(self, X_train, y_train_1d):
        self.model.fit(X_train, y_train_1d)

    def predict_proba_pos(self, X):
        proba = self.model.predict_proba(X)
        # handle rare single-class issues
        if proba.shape[1] == 1:
            # XGBoost normally returns 2 columns, but keep safe anyway
            return np.zeros((X.shape[0],), dtype=np.float32)
        return proba[:, 1].astype(np.float32)

# ----------------------------
# MGMOE (random gates, not trained)
# ----------------------------
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, input_dim: int, num_experts: int = 3, seed: int = 42):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        self.experts = [XGBExpert(seed=seed) for _ in range(num_experts)]
        self.gates = [
            tf.Variable(tf.random.uniform([input_dim, num_experts], seed=seed), trainable=True)
            for _ in range(num_tasks)
        ]

    def train_experts(self, X_train, y_train):
        # NOTE: trains expert i on task i (same pattern as your other scripts)
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def predict(self, X):
        out_experts = np.stack(
            [expert.predict_proba_pos(X) for expert in self.experts],
            axis=-1
        )  # (n, E)

        X_tf = tf.convert_to_tensor(X, dtype=tf.float32)

        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(X_tf, gate), axis=-1).numpy().astype(np.float32)  # (n, E)
            task_output = np.sum(out_experts * gate_scores, axis=-1).astype(np.float32)             # (n,)
            out_tasks.append(task_output)

        return np.stack(out_tasks, axis=-1).astype(np.float32)  # (n, T)

# ----------------------------
# Train once (no folds)
# ----------------------------
num_experts = 3
start = time.process_time()

np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    input_dim=X_train.shape[1],
    num_experts=num_experts,
    seed=GLOBAL_SEED
)
model.train_experts(X_train, y_train)

# ----------------------------
# Threshold tuning on VAL
# ----------------------------
y_val_prob = model.predict(X_val)
thr = best_thresholds(y_val, y_val_prob)

# ----------------------------
# Evaluate on TEST
# ----------------------------
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= thr).astype(np.int32)

assert y_test.shape == y_pred_labels.shape

f1_micro = f1_score(y_test, y_pred_labels, average='micro', zero_division=0) * 100
f1_macro = f1_score(y_test, y_pred_labels, average='macro', zero_division=0) * 100

print("\n================ FINAL TEST RESULTS ================")
print(f"F1-micro (sklearn): {f1_micro:.2f}")
print(f"F1-macro (sklearn): {f1_macro:.2f}")

for i in range(num_tasks):
    f1_task = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) * 100
    print(f"  Task {labels_name[i]} - F1: {f1_task:.2f}")

elapsed_cpu = time.process_time() - start
print(f"\nCPU time: {elapsed_cpu:.2f} s")

"""RF"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import tensorflow as tf

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['E', 'N', 'A', 'C', 'O']

# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Define labels for each task
y1 = labels['E']
y2 = labels['N']
y3 = labels['A']
y4 = labels['C']
y5 = labels['O']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# RandomForest Experts with updated parameters
class RandomForestExpert:
    def __init__(self, num_trees=300, max_depth=7, min_samples_split=3, min_samples_leaf=2, max_features='log2', ccp_alpha=0.01, criterion='entropy'):
        # Create a random forest classifier for each expert with the specified hyperparameters
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            random_state=100,  # Updated random_state as per your request
            n_jobs=-1,
            ccp_alpha=ccp_alpha,
            criterion=criterion  # Use 'entropy' criterion as requested
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Random Forest Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (RandomForest models) and gates for each task
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 4

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)


# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""DT"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score, precision_recall_curve
import tensorflow as tf
import time

# ----------------------------
# Reproducibility
# ----------------------------
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)

# ----------------------------
# Load and preprocess the data
# ----------------------------
data = pd.read_csv('/kaggle/input/personality-datasets1/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

labels = data[['E', 'N', 'A', 'C', 'O']].to_numpy(dtype=np.int32)

# Load BERT embeddings (CORRECT FILE)
embeddings = joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings, dtype=np.float32)

y = labels
num_tasks = y.shape[1]
labels_name = ['E', 'N', 'A', 'C', 'O']

# ----------------------------
# ONE split: 70 / 20 / 10
# ----------------------------
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.10, random_state=GLOBAL_SEED, shuffle=True
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=(2/9),
    random_state=GLOBAL_SEED,
    shuffle=True
)

print(f"Full:  {len(X)}")
print(f"Train: {len(X_train)} ({len(X_train)/len(X)*100:.2f}%)")
print(f"Val:   {len(X_val)} ({len(X_val)/len(X)*100:.2f}%)")
print(f"Test:  {len(X_test)} ({len(X_test)/len(X)*100:.2f}%)")

# ----------------------------
# Best threshold per label
# ----------------------------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)

    for i in range(y_true.shape[1]):
        yt = y_true[:, i]
        yp = y_prob[:, i]

        # If validation has only one class â†’ fallback
        if len(np.unique(yt)) < 2:
            thresholds[i] = 0.5
            continue

        p, r, th = precision_recall_curve(yt, yp)
        if th.size == 0:
            thresholds[i] = 0.5
            continue

        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]

    return thresholds

# ----------------------------
# Decision Tree Expert
# ----------------------------
class DecisionTreeExpert:
    def __init__(self,
                 criterion='log_loss',
                 splitter='best',
                 max_depth=8,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 random_state=42,
                 ccp_alpha=0.1):
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            random_state=random_state,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train_1d):
        self.model.fit(X_train, y_train_1d)

    def predict_proba_pos(self, X):
        proba = self.model.predict_proba(X)
        if proba.shape[1] == 1:
            cls = self.model.classes_[0]
            return np.ones((X.shape[0],), dtype=np.float32) if cls == 1 else np.zeros((X.shape[0],), dtype=np.float32)
        return proba[:, 1].astype(np.float32)

# ----------------------------
# MGMOE (random, untrained gates)
# ----------------------------
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, input_dim: int, num_experts: int = 3, seed: int = 42):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        self.experts = [DecisionTreeExpert(random_state=seed) for _ in range(num_experts)]
        self.gates = [
            tf.Variable(tf.random.uniform([input_dim, num_experts], seed=seed), trainable=True)
            for _ in range(num_tasks)
        ]

    def train_experts(self, X_train, y_train):
        # Same behavior as your original code
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def predict(self, X):
        out_experts = np.stack(
            [expert.predict_proba_pos(X) for expert in self.experts],
            axis=-1
        )  # (n, E)

        X_tf = tf.convert_to_tensor(X, dtype=tf.float32)

        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(X_tf, gate), axis=-1).numpy()
            task_output = np.sum(out_experts * gate_scores, axis=-1)
            out_tasks.append(task_output)

        return np.stack(out_tasks, axis=-1)  # (n, T)

# ----------------------------
# Train ONCE
# ----------------------------
start = time.process_time()

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    input_dim=X_train.shape[1],
    num_experts=3,
    seed=GLOBAL_SEED
)

model.train_experts(X_train, y_train)

# ----------------------------
# Threshold tuning on VAL
# ----------------------------
y_val_prob = model.predict(X_val)
thr = best_thresholds(y_val, y_val_prob)


# ----------------------------
# Evaluate on TEST (BEST THR)
# ----------------------------
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= thr).astype(int)

f1_micro = f1_score(y_test, y_pred_labels, average='micro', zero_division=0) * 100
f1_macro = f1_score(y_test, y_pred_labels, average='macro', zero_division=0) * 100

print("\n================ FINAL TEST RESULTS ================")
print(f"F1-micro: {f1_micro:.2f}")
print(f"F1-macro: {f1_macro:.2f}")

for i, name in enumerate(labels_name):
    f1_task = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) * 100
    print(f"  Task {name} - F1: {f1_task:.2f}")

elapsed_cpu = time.process_time() - start
print(f"\nCPU time: {elapsed_cpu:.2f} s")

"""SVC"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score
import tensorflow as tf

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess the data
data = pd.read_csv('/content/AraPersonality_with_text.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_512_128_arabic.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['E', 'N', 'A', 'C', 'O']

# Extract text data and labels
texts = data['text']
labels = data[['E', 'N', 'A', 'C', 'O']]

# Define labels for each task
y1 = labels['E']
y2 = labels['N']
y3 = labels['A']
y4 = labels['C']
y5 = labels['O']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Support Vector Classifier (SVC) Expert Class
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001, class_weight=None, random_state=42, max_iter=-1):
        # Create a support vector classifier for each expert with optimized hyperparameters
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True  # Enable probability estimates
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Support Vector Classifier provides probability estimates if probability=True
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with SVC Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (SVC models) and gates for each task
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)

# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")