# -*- coding: utf-8 -*-
"""friendspersona_mtl_upload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f5dFMiAV2j1eLsHEydlFkWyEqPPqIat3

#Multi-task Learning using Mixture of Experts (MGMOE) with BERT (DONE)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy
from sklearn.metrics import f1_score, precision_recall_curve  # <-- added
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
# Extract labels
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
y = labels.to_numpy()

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        loss = tf.reduce_sum(loss)
        return loss

# Transformer Block (replaces CNN)
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Expert using Transformer instead of CNN
class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # Add sequence dimension
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3, num_heads: int = 4, ff_dim: int = 128, dropout_rate: float = 0.1):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False) for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, hidden_dim)
        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)  # (batch_size, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)  # Correct einsum
            output = tower(mixed_expert, training=training)
            task_outputs.append(output)
        out = tf.concat(task_outputs, axis=-1)
        out = tf.nn.sigmoid(out)
        return out

# Create and compile the model
num_tasks = 5
embed_dim = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.01
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)

# -----------------------------
# Per-label threshold tuning
# -----------------------------
# 1) Get validation probabilities
y_val_prob = model.predict(X_val)

# 2) Find best threshold per label on validation set (max F1)
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5  # fallback
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# 3) Apply thresholds on test probabilities
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= thr).astype(int)

# Save y_pred_labels and thresholds
joblib.dump(y_pred_labels, 'y_pred_labels_transformer_MTL_F_thresholded.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -----------------------------
# Evaluation (unchanged logic, now using thresholded preds)
# -----------------------------
# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0
print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])
print(f"Manually calculated F1-macro: {f1_macro_manual}")
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)
print(f"F1-micro : {f1_micro_sklearn}")
print(f"F1-macro : {f1_macro_sklearn}")
# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy
from sklearn.metrics import f1_score, precision_recall_curve  # <-- added
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
# Extract labels
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
y = labels.to_numpy()

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        loss = tf.reduce_sum(loss)
        return loss

# Transformer Block (replaces CNN)
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Expert using Transformer instead of CNN
class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # Add sequence dimension
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3, num_heads: int = 4, ff_dim: int = 128, dropout_rate: float = 0.1):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False) for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, hidden_dim)
        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)  # (batch_size, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)  # Correct einsum
            output = tower(mixed_expert, training=training)
            task_outputs.append(output)
        out = tf.concat(task_outputs, axis=-1)
        out = tf.nn.sigmoid(out)
        return out

# Create and compile the model
num_tasks = 5
embed_dim = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.01
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)

# -----------------------------
# Per-label threshold tuning
# -----------------------------
# 1) Get validation probabilities
y_val_prob = model.predict(X_val)

# 2) Find best threshold per label on validation set (max F1)
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5  # fallback
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# 3) Apply thresholds on test probabilities
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= thr).astype(int)

# Save y_pred_labels and thresholds
joblib.dump(y_pred_labels, 'y_pred_labels_transformer_MTL_BERT.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -----------------------------
# Evaluation (unchanged logic, now using thresholded preds)
# -----------------------------
# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])
print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"F1-micro : {f1_micro_sklearn}")
print(f"F1-macro : {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy
from sklearn.metrics import f1_score, precision_recall_curve  # <-- added
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)

# List of personality traits (labels)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
# Extract labels
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
y = labels.to_numpy()

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        loss = tf.reduce_sum(loss)
        return loss

# Transformer Block (replaces CNN)
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Expert using Transformer instead of CNN
class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # Add sequence dimension
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3, num_heads: int = 4, ff_dim: int = 128, dropout_rate: float = 0.1):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False) for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, hidden_dim)
        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)  # (batch_size, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)  # Correct einsum
            output = tower(mixed_expert, training=training)
            task_outputs.append(output)
        out = tf.concat(task_outputs, axis=-1)
        out = tf.nn.sigmoid(out)
        return out

# Create and compile the model
num_tasks = 5
embed_dim = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.01
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)
y_val_prob = model.predict(X_val)
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5  # fallback
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# 3) Apply thresholds on test probabilities
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= thr).astype(int)

# Save y_pred_labels and thresholds
joblib.dump(y_pred_labels, 'y_pred_labels_transformer_MTL_BERT.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -----------------------------
# Evaluation (unchanged logic, now using thresholded preds)
# -----------------------------
# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])
print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"F1-micro : {f1_micro_sklearn}")
print(f"F1-macro : {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with CNN (Done)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve
import tensorflow as tf
from keras import Sequential, Model, regularizers
from keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.losses import Loss
from keras.metrics import BinaryAccuracy, AUC
# -------- Reproducibility --------
np.random.seed(42)
tf.random.set_seed(42)
# -------- Data --------
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']].to_numpy(dtype=np.float32)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
embeddings = joblib.load('bert_embeddings_full_clean.pkl')
X = np.array(embeddings, dtype=np.float32)

X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

num_tasks = y_train.shape[1]
num_embeddings = X_train.shape[1]

# -------- Class imbalance: per-task positive weights --------
eps = 1e-8
pos_counts = np.sum(y_train, axis=0)
neg_counts = y_train.shape[0] - pos_counts
pos_weight = (neg_counts + eps) / (pos_counts + eps)  # shape (num_tasks,)
pos_weight_tf = tf.constant(pos_weight, dtype=tf.float32)

# -------- Weighted BCE on probabilities --------
class MultiTaskWeightedBCE(Loss):
    def __init__(self, pos_weight_vec):
        super().__init__()
        self.pos_weight = tf.reshape(pos_weight_vec, (1, -1))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)
        pos_term = - self.pos_weight * y_true * tf.math.log(y_pred)
        neg_term = - (1. - y_true) * tf.math.log(1. - y_pred)
        return tf.reduce_mean(tf.reduce_sum(pos_term + neg_term, axis=1))

# -------- CNN block (with optional L2) --------
class CNN(Sequential):
    def __init__(self, num_conv_layers: int, filters: int, kernel_size: int, pool_size: int,
                 dense_units: int, dim_out: int = None, dropout: float = 0.0, l2=1e-4, name: str = "CNN"):
        layers = []
        layers.append(Reshape((-1, 1)))  # treat embedding as a 1D "sequence"

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',
                                 kernel_regularizer=regularizers.l2(l2)))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2)))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))  # logits; we'll apply sigmoid outside

        super().__init__(layers, name=name)

# -------- MG-MoE --------
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, num_emb: int, num_experts: int = 3,
                 num_conv_layers_expert: int = 3, filters_expert: int = 64, kernel_size_expert: int = 3,
                 pool_size_expert: int = 2, dense_units_expert: int = 64, dropout_expert: float = 0.1, l2_expert=1e-4,
                 gate_function: str = "softmax",
                 num_conv_layers_tasks: int = 2, filters_tasks: int = 64, kernel_size_tasks: int = 3,
                 pool_size_tasks: int = 2, dense_units_tasks: int = 64, dim_out_tasks: int = 1,
                 dropout_tasks: float = 0.1, l2_tasks=1e-4):
        super().__init__()
        # Experts
        self.experts = [
            CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert,
                dense_units_expert, dropout=dropout_expert, l2=l2_expert)
            for _ in range(num_experts)
        ]
        # Towers + Gates per task
        self.towers = [
            CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks,
                dense_units_tasks, dim_out_tasks, dropout=dropout_tasks, l2=l2_tasks)
            for _ in range(num_tasks)
        ]
        self.gates = [Dense(num_experts, activation=gate_function, use_bias=False) for _ in range(num_tasks)]

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts
        out_experts = []
        expert_input = tf.expand_dims(inputs, -1)  # (bs, D) -> (bs, D, 1)
        for expert in self.experts:
            out_experts.append(expert(expert_input, training=training))  # (bs, H)
        out_experts = tf.stack(out_experts, axis=-1)  # (bs, H, E)

        # Tasks
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)              # (bs, E)
            mixed = tf.einsum("bhe,be->bh", out_experts, gate_score)  # (bs, H)
            logits = tower(tf.expand_dims(mixed, -1), training=training)  # (bs, 1)
            out_tasks.append(logits)

        out = tf.concat(out_tasks, -1)  # (bs, num_tasks) logits
        out = tf.nn.sigmoid(out)        # probabilities
        return out

# -------- Build / compile --------
model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    num_experts=3,
    num_conv_layers_expert=3, filters_expert=64, kernel_size_expert=3, pool_size_expert=2,
    dense_units_expert=64, dropout_expert=0.1, l2_expert=1e-4,
    gate_function="softmax",
    num_conv_layers_tasks=2, filters_tasks=64, kernel_size_tasks=3, pool_size_tasks=2,
    dense_units_tasks=64, dim_out_tasks=1, dropout_tasks=0.1, l2_tasks=1e-4
)

loss = MultiTaskWeightedBCE(pos_weight_tf)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=optimizer, loss=loss,
              metrics=[BinaryAccuracy(name="bin_acc"), AUC(name="auc", multi_label=True)])

# -------- Callbacks --------
early_stop = tf.keras.callbacks.EarlyStopping(monitor="val_auc", mode="max", patience=7, restore_best_weights=True)
reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max", factor=0.6, patience=3, min_lr=1e-6, verbose=1)

# -------- Train --------
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=60, batch_size=64,
                    callbacks=[early_stop, reduce_lr],
                    verbose=2)

# -------- Predict probas --------
y_val_prob = model.predict(X_val)
y_test_prob = model.predict(X_test)

# -------- Threshold tuning on validation --------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# -------- Apply thresholds to test --------
y_pred_labels = (y_test_prob >= thr).astype(int)

# -------- Save outputs --------
joblib.dump(y_pred_labels, 'y_pred_labels_MTL_CNN_full_clean.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -------- Evaluate --------
tp = fp = fn = 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0.0
recall_micro_manual    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
f1_micro_manual        = 2 * precision_micro_manual * recall_micro_manual / (precision_micro_manual + recall_micro_manual + 1e-12)
f1_macro_manual        = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])

f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"Manually calculated F1-micro: {f1_micro_manual:.4f}")
print(f"Manually calculated F1-macro: {f1_macro_manual:.4f}")
print(f"F1-micro : {f1_micro_sklearn:.4f}")
print(f"F1-macro : {f1_macro_sklearn:.4f}")

for i, name in enumerate(traits):
    f1_i = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {name} - F1-Score: {f1_i:.4f}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve
import tensorflow as tf
from keras import Sequential, Model, regularizers
from keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.losses import Loss
from keras.metrics import BinaryAccuracy, AUC
# -------- Reproducibility --------
np.random.seed(42)
tf.random.set_seed(42)
# -------- Data --------
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']].to_numpy(dtype=np.float32)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
embeddings = joblib.load('bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings, dtype=np.float32)

X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

num_tasks = y_train.shape[1]
num_embeddings = X_train.shape[1]

# -------- Class imbalance: per-task positive weights --------
eps = 1e-8
pos_counts = np.sum(y_train, axis=0)
neg_counts = y_train.shape[0] - pos_counts
pos_weight = (neg_counts + eps) / (pos_counts + eps)  # shape (num_tasks,)
pos_weight_tf = tf.constant(pos_weight, dtype=tf.float32)

# -------- Weighted BCE on probabilities --------
class MultiTaskWeightedBCE(Loss):
    def __init__(self, pos_weight_vec):
        super().__init__()
        self.pos_weight = tf.reshape(pos_weight_vec, (1, -1))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)
        pos_term = - self.pos_weight * y_true * tf.math.log(y_pred)
        neg_term = - (1. - y_true) * tf.math.log(1. - y_pred)
        return tf.reduce_mean(tf.reduce_sum(pos_term + neg_term, axis=1))

# -------- CNN block (with optional L2) --------
class CNN(Sequential):
    def __init__(self, num_conv_layers: int, filters: int, kernel_size: int, pool_size: int,
                 dense_units: int, dim_out: int = None, dropout: float = 0.0, l2=1e-4, name: str = "CNN"):
        layers = []
        layers.append(Reshape((-1, 1)))  # treat embedding as a 1D "sequence"

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',
                                 kernel_regularizer=regularizers.l2(l2)))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2)))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))  # logits; we'll apply sigmoid outside

        super().__init__(layers, name=name)

# -------- MG-MoE --------
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, num_emb: int, num_experts: int = 3,
                 num_conv_layers_expert: int = 3, filters_expert: int = 64, kernel_size_expert: int = 3,
                 pool_size_expert: int = 2, dense_units_expert: int = 64, dropout_expert: float = 0.1, l2_expert=1e-4,
                 gate_function: str = "softmax",
                 num_conv_layers_tasks: int = 2, filters_tasks: int = 64, kernel_size_tasks: int = 3,
                 pool_size_tasks: int = 2, dense_units_tasks: int = 64, dim_out_tasks: int = 1,
                 dropout_tasks: float = 0.1, l2_tasks=1e-4):
        super().__init__()
        # Experts
        self.experts = [
            CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert,
                dense_units_expert, dropout=dropout_expert, l2=l2_expert)
            for _ in range(num_experts)
        ]
        # Towers + Gates per task
        self.towers = [
            CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks,
                dense_units_tasks, dim_out_tasks, dropout=dropout_tasks, l2=l2_tasks)
            for _ in range(num_tasks)
        ]
        self.gates = [Dense(num_experts, activation=gate_function, use_bias=False) for _ in range(num_tasks)]

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts
        out_experts = []
        expert_input = tf.expand_dims(inputs, -1)  # (bs, D) -> (bs, D, 1)
        for expert in self.experts:
            out_experts.append(expert(expert_input, training=training))  # (bs, H)
        out_experts = tf.stack(out_experts, axis=-1)  # (bs, H, E)

        # Tasks
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)              # (bs, E)
            mixed = tf.einsum("bhe,be->bh", out_experts, gate_score)  # (bs, H)
            logits = tower(tf.expand_dims(mixed, -1), training=training)  # (bs, 1)
            out_tasks.append(logits)

        out = tf.concat(out_tasks, -1)  # (bs, num_tasks) logits
        out = tf.nn.sigmoid(out)        # probabilities
        return out

# -------- Build / compile --------
model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    num_experts=3,
    num_conv_layers_expert=3, filters_expert=64, kernel_size_expert=3, pool_size_expert=2,
    dense_units_expert=64, dropout_expert=0.1, l2_expert=1e-4,
    gate_function="softmax",
    num_conv_layers_tasks=2, filters_tasks=64, kernel_size_tasks=3, pool_size_tasks=2,
    dense_units_tasks=64, dim_out_tasks=1, dropout_tasks=0.1, l2_tasks=1e-4
)

loss = MultiTaskWeightedBCE(pos_weight_tf)
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)
model.compile(optimizer=optimizer, loss=loss,
              metrics=[BinaryAccuracy(name="bin_acc"), AUC(name="auc", multi_label=True)])

# -------- Callbacks --------
early_stop = tf.keras.callbacks.EarlyStopping(monitor="val_auc", mode="max", patience=7, restore_best_weights=True)
reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max", factor=0.6, patience=3, min_lr=1e-6, verbose=1)

# -------- Train --------
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=60, batch_size=64,
                    callbacks=[early_stop, reduce_lr],
                    verbose=2)

# -------- Predict probas --------
y_val_prob = model.predict(X_val)
y_test_prob = model.predict(X_test)

# -------- Threshold tuning on validation --------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# -------- Apply thresholds to test --------
y_pred_labels = (y_test_prob >= thr).astype(int)

# -------- Save outputs --------
joblib.dump(y_pred_labels, 'y_pred_labels_MTL_CNN_single_context_clean.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -------- Evaluate --------
tp = fp = fn = 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0.0
recall_micro_manual    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
f1_micro_manual        = 2 * precision_micro_manual * recall_micro_manual / (precision_micro_manual + recall_micro_manual + 1e-12)
f1_macro_manual        = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])

f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"Manually calculated F1-micro: {f1_micro_manual:.4f}")
print(f"Manually calculated F1-macro: {f1_macro_manual:.4f}")
print(f"F1-micro : {f1_micro_sklearn:.4f}")
print(f"F1-macro : {f1_macro_sklearn:.4f}")

for i, name in enumerate(traits):
    f1_i = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {name} - F1-Score: {f1_i:.4f}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve
import tensorflow as tf
from keras import Sequential, Model, regularizers
from keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.losses import Loss
from keras.metrics import BinaryAccuracy, AUC
# -------- Reproducibility --------
np.random.seed(42)
tf.random.set_seed(42)
# -------- Data --------
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']].to_numpy(dtype=np.float32)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
embeddings = joblib.load('bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings, dtype=np.float32)

X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

num_tasks = y_train.shape[1]
num_embeddings = X_train.shape[1]

# -------- Class imbalance: per-task positive weights --------
eps = 1e-8
pos_counts = np.sum(y_train, axis=0)
neg_counts = y_train.shape[0] - pos_counts
pos_weight = (neg_counts + eps) / (pos_counts + eps)  # shape (num_tasks,)
pos_weight_tf = tf.constant(pos_weight, dtype=tf.float32)

# -------- Weighted BCE on probabilities --------
class MultiTaskWeightedBCE(Loss):
    def __init__(self, pos_weight_vec):
        super().__init__()
        self.pos_weight = tf.reshape(pos_weight_vec, (1, -1))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)
        pos_term = - self.pos_weight * y_true * tf.math.log(y_pred)
        neg_term = - (1. - y_true) * tf.math.log(1. - y_pred)
        return tf.reduce_mean(tf.reduce_sum(pos_term + neg_term, axis=1))

# -------- CNN block (with optional L2) --------
class CNN(Sequential):
    def __init__(self, num_conv_layers: int, filters: int, kernel_size: int, pool_size: int,
                 dense_units: int, dim_out: int = None, dropout: float = 0.0, l2=1e-4, name: str = "CNN"):
        layers = []
        layers.append(Reshape((-1, 1)))  # treat embedding as a 1D "sequence"

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',
                                 kernel_regularizer=regularizers.l2(l2)))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(l2)))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))  # logits; we'll apply sigmoid outside

        super().__init__(layers, name=name)

# -------- MG-MoE --------
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, num_emb: int, num_experts: int = 3,
                 num_conv_layers_expert: int = 3, filters_expert: int = 64, kernel_size_expert: int = 3,
                 pool_size_expert: int = 2, dense_units_expert: int = 64, dropout_expert: float = 0.1, l2_expert=1e-4,
                 gate_function: str = "softmax",
                 num_conv_layers_tasks: int = 2, filters_tasks: int = 64, kernel_size_tasks: int = 3,
                 pool_size_tasks: int = 2, dense_units_tasks: int = 64, dim_out_tasks: int = 1,
                 dropout_tasks: float = 0.1, l2_tasks=1e-4):
        super().__init__()
        # Experts
        self.experts = [
            CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert,
                dense_units_expert, dropout=dropout_expert, l2=l2_expert)
            for _ in range(num_experts)
        ]
        # Towers + Gates per task
        self.towers = [
            CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks,
                dense_units_tasks, dim_out_tasks, dropout=dropout_tasks, l2=l2_tasks)
            for _ in range(num_tasks)
        ]
        self.gates = [Dense(num_experts, activation=gate_function, use_bias=False) for _ in range(num_tasks)]

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts
        out_experts = []
        expert_input = tf.expand_dims(inputs, -1)  # (bs, D) -> (bs, D, 1)
        for expert in self.experts:
            out_experts.append(expert(expert_input, training=training))  # (bs, H)
        out_experts = tf.stack(out_experts, axis=-1)  # (bs, H, E)

        # Tasks
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)              # (bs, E)
            mixed = tf.einsum("bhe,be->bh", out_experts, gate_score)  # (bs, H)
            logits = tower(tf.expand_dims(mixed, -1), training=training)  # (bs, 1)
            out_tasks.append(logits)

        out = tf.concat(out_tasks, -1)  # (bs, num_tasks) logits
        out = tf.nn.sigmoid(out)        # probabilities
        return out

# -------- Build / compile --------
model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    num_experts=3,
    num_conv_layers_expert=3, filters_expert=64, kernel_size_expert=3, pool_size_expert=2,
    dense_units_expert=64, dropout_expert=0.1, l2_expert=1e-4,
    gate_function="softmax",
    num_conv_layers_tasks=2, filters_tasks=64, kernel_size_tasks=3, pool_size_tasks=2,
    dense_units_tasks=64, dim_out_tasks=1, dropout_tasks=0.1, l2_tasks=1e-4
)

loss = MultiTaskWeightedBCE(pos_weight_tf)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=optimizer, loss=loss,
              metrics=[BinaryAccuracy(name="bin_acc"), AUC(name="auc", multi_label=True)])

# -------- Callbacks --------
early_stop = tf.keras.callbacks.EarlyStopping(monitor="val_auc", mode="max", patience=7, restore_best_weights=True)
reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_auc", mode="max", factor=0.6, patience=3, min_lr=1e-6, verbose=1)

# -------- Train --------
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=60, batch_size=64,
                    callbacks=[early_stop, reduce_lr],
                    verbose=2)

# -------- Predict probas --------
y_val_prob = model.predict(X_val)
y_test_prob = model.predict(X_test)

# -------- Threshold tuning on validation --------
def best_thresholds(y_true, y_prob):
    thresholds = np.zeros(y_true.shape[1], dtype=np.float32)
    for i in range(y_true.shape[1]):
        p, r, th = precision_recall_curve(y_true[:, i], y_prob[:, i])
        if th.size == 0:
            thresholds[i] = 0.5
            continue
        f1 = 2 * p[:-1] * r[:-1] / (p[:-1] + r[:-1] + 1e-12)
        thresholds[i] = th[np.nanargmax(f1)]
    return thresholds

thr = best_thresholds(y_val, y_val_prob)
print("Per-label thresholds:", dict(zip(traits, np.round(thr, 4))))

# -------- Apply thresholds to test --------
y_pred_labels = (y_test_prob >= thr).astype(int)

# -------- Save outputs --------
joblib.dump(y_pred_labels, 'y_pred_labels_MTL_CNN_single_text_clean.pkl')
joblib.dump(thr, 'val_tuned_thresholds.pkl')

# -------- Evaluate --------
tp = fp = fn = 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0.0
recall_micro_manual    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
f1_micro_manual        = 2 * precision_micro_manual * recall_micro_manual / (precision_micro_manual + recall_micro_manual + 1e-12)
f1_macro_manual        = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])

f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"Manually calculated F1-micro: {f1_micro_manual:.4f}")
print(f"Manually calculated F1-macro: {f1_macro_manual:.4f}")
print(f"F1-micro : {f1_micro_sklearn:.4f}")
print(f"F1-macro : {f1_macro_sklearn:.4f}")

for i, name in enumerate(traits):
    f1_i = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {name} - F1-Score: {f1_i:.4f}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with MLP (Done)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# MLP Experts (replacing Logistic Regression)
class MLPExpert:
    def __init__(self, hidden_layer_sizes=(256, 32), activation='relu', alpha=1e-4, max_iter=150):
        # Create an MLP classifier for each expert
        self.model = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation=activation,
            solver='adam',
            alpha=alpha,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.02,
            max_iter=max_iter,
            shuffle=True,
            random_state=42,
            early_stopping=True,
            n_iter_no_change=10,
            verbose=False
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Return the probability of the positive class
        return self.model.predict_proba(X)[:, 1]

# MGMOE Model with MLP Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (MLP models) and gates for each task
        self.experts = [MLPExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (probabilities)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('friends_preprocessed.csv')
data.dropna(inplace=True)
#print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# MLP Experts (replacing Logistic Regression)
class MLPExpert:
    def __init__(self, hidden_layer_sizes=(256,32), activation='relu', alpha=1e-4, max_iter=200):
        # Create an MLP classifier for each expert
        self.model = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation=activation,
            solver='adam',
            alpha=alpha,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.02,
            max_iter=max_iter,
            shuffle= True,
            random_state=42,
            early_stopping=True,
            n_iter_no_change=10,
            verbose=False
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Return the probability of the positive class
        return self.model.predict_proba(X)[:, 1]

# MGMOE Model with MLP Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (MLP models) and gates for each task
        self.experts = [MLPExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (probabilities)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# MLP Experts (replacing Logistic Regression)
class MLPExpert:
    def __init__(self, hidden_layer_sizes=(256,32), activation='relu', alpha=1e-4, max_iter=150):
        # Create an MLP classifier for each expert
        self.model = MLPClassifier(
            hidden_layer_sizes=hidden_layer_sizes,
            activation=activation,
            solver='adam',
            alpha=alpha,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.02,
            max_iter=max_iter,
            shuffle=True,
            random_state=42,
            early_stopping=True,
            n_iter_no_change=10,
            verbose=False
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Return the probability of the positive class
        return self.model.predict_proba(X)[:, 1]

# MGMOE Model with MLP Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (MLP models) and gates for each task
        self.experts = [MLPExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (probabilities)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with Random Forest Models (Done)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_full_clean.pkl')
X = np.array(embeddings)
# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# RandomForest Experts
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=4, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        # Create a random forest classifier for each expert with optimized hyperparameters
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Random Forest Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (RandomForest models) and gates for each task
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# RandomForest Experts
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=4, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        # Create a random forest classifier for each expert with optimized hyperparameters
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Random Forest Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (RandomForest models) and gates for each task
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# RandomForest Experts
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=5, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        # Create a random forest classifier for each expert with optimized hyperparameters
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Random Forest Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (RandomForest models) and gates for each task
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""#Logistic Regression (DONE)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Logistic Regression Experts
class LogisticRegressionExpert:
    def __init__(self, C=0.00001, max_iter=300):
        # Create a logistic regression classifier for each expert with optimized hyperparameters
        self.model = LogisticRegression(
            random_state=42,
            max_iter=max_iter,
            C=C,
            fit_intercept=True
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Logistic Regression Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (LogisticRegression models) and gates for each task
        self.experts = [LogisticRegressionExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Logistic Regression Experts
class LogisticRegressionExpert:
    def __init__(self, C=0.00001, max_iter=200, solver='lbfgs'):
        # Create a logistic regression classifier for each expert with optimized hyperparameters
        self.model = LogisticRegression(
            random_state=42,
            max_iter=max_iter,
            C=C,
            solver=solver,
            fit_intercept=True
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Logistic Regression Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (LogisticRegression models) and gates for each task
        self.experts = [LogisticRegressionExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)
# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Logistic Regression Experts
class LogisticRegressionExpert:
    def __init__(self, C=0.00001, max_iter=200, solver='lbfgs'):
        # Create a logistic regression classifier for each expert with optimized hyperparameters
        self.model = LogisticRegression(
            random_state=42,
            max_iter=max_iter,
            C=C,
            solver=solver,
            fit_intercept=True
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with Logistic Regression Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (LogisticRegression models) and gates for each task
        self.experts = [LogisticRegressionExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with Support Vector Classifiers (SVC) (Done)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Support Vector Classifier (SVC) Expert Class
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001, class_weight=None, random_state=42, max_iter=-1):
        # Create a support vector classifier for each expert with optimized hyperparameters
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True  # Enable probability estimates
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Support Vector Classifier provides probability estimates if probability=True
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with SVC Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (SVC models) and gates for each task
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)
# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)
# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Support Vector Classifier (SVC) Expert Class
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001, class_weight=None, random_state=42, max_iter=-1):
        # Create a support vector classifier for each expert with optimized hyperparameters
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True  # Enable probability estimates
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Support Vector Classifier provides probability estimates if probability=True
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with SVC Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (SVC models) and gates for each task
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Support Vector Classifier (SVC) Expert Class
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001, class_weight=None, random_state=42, max_iter=-1):
        # Create a support vector classifier for each expert with optimized hyperparameters
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True  # Enable probability estimates
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Support Vector Classifier provides probability estimates if probability=True
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with SVC Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (SVC models) and gates for each task
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with  Decision Trees Models (Done)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Decision Tree Expert Class with optimized parameters
class DecisionTreeExpert:
    def __init__(self, criterion='log_loss', splitter='best', max_depth=5, min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features=None, random_state=42, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.1):
        # Create a decision tree classifier for each expert with optimized hyperparameters
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # DecisionTree supports predict_proba if it's a classification task
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class
        else:
            return self.model.predict(X)

# MGMOE Model with Decision Tree Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (DecisionTree models) and gates for each task
        self.experts = [DecisionTreeExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Decision Tree Expert Class with optimized parameters
class DecisionTreeExpert:
    def __init__(self, criterion='log_loss', splitter='best', max_depth=5, min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features=None, random_state=42, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.1):
        # Create a decision tree classifier for each expert with optimized hyperparameters
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # DecisionTree supports predict_proba if it's a classification task
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class
        else:
            return self.model.predict(X)

# MGMOE Model with Decision Tree Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (DecisionTree models) and gates for each task
        self.experts = [DecisionTreeExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Decision Tree Expert Class with optimized parameters
class DecisionTreeExpert:
    def __init__(self, criterion='log_loss', splitter='best', max_depth=5, min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features=None, random_state=42, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.1):
        # Create a decision tree classifier for each expert with optimized hyperparameters
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # DecisionTree supports predict_proba if it's a classification task
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class
        else:
            return self.model.predict(X)

# MGMOE Model with Decision Tree Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (DecisionTree models) and gates for each task
        self.experts = [DecisionTreeExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with XGBClassifier (XGB) (Done two datasets)

**full_clean**
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from xgboost import XGBClassifier
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['full_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_full_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Replace -1 labels with 0 for binary classification (or map classes appropriately)
y = np.where(y == -1, 0, y)
# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# XGBClassifier Expert Class
class XGBExpert:
    def __init__(self):
        # Create an XGBClassifier with optimized hyperparameters
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.005,
            objective='binary:logistic',
            # use_label_encoder=False,
            eval_metric='mlogloss',
            random_state=42
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # XGBClassifier provides probability estimates
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with XGB Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (XGB models) and gates for each task
        self.experts = [XGBExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_context_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from xgboost import XGBClassifier
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_context_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_context_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Replace -1 labels with 0 for binary classification (or map classes appropriately)
y = np.where(y == -1, 0, y)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# XGBClassifier Expert Class
class XGBExpert:
    def __init__(self):
        # Create an XGBClassifier with optimized hyperparameters
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.005,
            objective='binary:logistic',
            # use_label_encoder=False,
            eval_metric='mlogloss',
            random_state=42
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # XGBClassifier provides probability estimates
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with XGB Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (XGB models) and gates for each task
        self.experts = [XGBExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')
print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

"""**single_text_clean**"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from xgboost import XGBClassifier
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/friends_preprocessed.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['single_text_clean']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_single_text_clean.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Replace -1 labels with 0 for binary classification (or map classes appropriately)
y = np.where(y == -1, 0, y)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# XGBClassifier Expert Class
class XGBExpert:
    def __init__(self):
        # Create an XGBClassifier with optimized hyperparameters
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=5,
            learning_rate=0.005,
            objective='binary:logistic',
            # use_label_encoder=False,
            eval_metric='mlogloss',
            random_state=42
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # XGBClassifier provides probability estimates
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with XGB Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (XGB models) and gates for each task
        self.experts = [XGBExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")