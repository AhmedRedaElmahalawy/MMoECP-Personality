# -*- coding: utf-8 -*-
"""essays_mtl_upload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZe3m40Y_ljb1fGbur1Vc8H_tp8Z76Fl

#Multi-task Learning using Mixture of Experts (MGMOE) with BERT
"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy
from sklearn.metrics import f1_score, precision_recall_curve  # <-- added
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/kaggle/input/personality-datasets1/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Load the saved BERT embeddings
embeddings = joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_essays.pkl')
X = np.array(embeddings)
# List of personality traits (labels)
traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
# Extract labels
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
y = labels.to_numpy()

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=0.02) -> None:
        super().__init__()
        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)
        loss = tf.reduce_sum(loss)
        return loss

# Transformer Block (replaces CNN)
class TransformerBlock(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-2)
        self.layernorm2 = LayerNormalization(epsilon=1e-2)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Expert using Transformer instead of CNN
class ExpertTransformer(Model):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)
        self.pool = GlobalAveragePooling1D()
        self.dense = Dense(ff_dim, activation="relu")
        self.dropout = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        x = tf.expand_dims(inputs, 1)  # Add sequence dimension
        x = self.transformer_block(x, training=training)
        x = self.pool(x)
        x = self.dense(x)
        x = self.dropout(x, training=training)
        return x

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, embed_dim: int, num_experts: int = 3, num_heads: int = 4, ff_dim: int = 128, dropout_rate: float = 0.1):
        super().__init__()
        self.experts = [ExpertTransformer(embed_dim, num_heads, ff_dim, dropout_rate) for _ in range(num_experts)]
        self.gates = [Dense(num_experts, activation="softmax", use_bias=False) for _ in range(num_tasks)]
        self.towers = [Sequential([
            Dense(64, activation="relu"),
            Dropout(dropout_rate),
            Dense(1)
        ]) for _ in range(num_tasks)]

    def call(self, inputs, training=False):
        expert_outputs = [expert(inputs, training=training) for expert in self.experts]
        expert_outputs = tf.stack(expert_outputs, axis=1)  # (batch_size, num_experts, hidden_dim)
        task_outputs = []
        for gate, tower in zip(self.gates, self.towers):
            gate_scores = gate(inputs)  # (batch_size, num_experts)
            mixed_expert = tf.einsum('bnd,bn->bd', expert_outputs, gate_scores)  # Correct einsum
            output = tower(mixed_expert, training=training)
            task_outputs.append(output)
        out = tf.concat(task_outputs, axis=-1)
        out = tf.nn.sigmoid(out)
        return out

# Create and compile the model
num_tasks = 5
embed_dim = X_train.shape[1]

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    embed_dim=embed_dim,
    num_experts=3,
    num_heads=8,
    ff_dim=64,
    dropout_rate=0.01
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=32)
# 1) Get validation probabilities
y_val_prob = model.predict(X_val)
y_test_prob = model.predict(X_test)
y_pred_labels = (y_test_prob >= 0.5).astype(int)


# -----------------------------
# Evaluation (unchanged logic, now using thresholded preds)
# -----------------------------
# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) for i in range(num_tasks)])
print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro', zero_division=0)
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro', zero_division=0)

print(f"F1-micro : {f1_micro_sklearn}")
print(f"F1-macro : {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with CNN (Done)"""

import time
import numpy as np
import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve

import tensorflow as tf
from keras import Sequential, Model, regularizers
from keras.layers import Dense, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten, Reshape
from keras.losses import Loss
from keras.metrics import BinaryAccuracy, AUC
# -----------------------------
# Reproducibility
# -----------------------------
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
tf.random.set_seed(GLOBAL_SEED)
# -----------------------------
# Load data
# -----------------------------
data = pd.read_csv('/kaggle/input/personality-datasets1/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
y_raw = data[traits].to_numpy()  # may be non-binary
X_all = np.array(joblib.load('/kaggle/input/personality-datasets1/bert_embeddings_essays.pkl'), dtype=np.float32)

num_tasks = len(traits)
# -----------------------------
# Helper: make labels binary
# -----------------------------
def to_binary_labels(y):
    """
    Converts labels to {0,1}.
    Works for cases like {-1, 1}, {0,1}, {-1,0,1}, or continuous.
    Rule:
      - if labels contain -1 => map (-1 -> 0), keep others, then clip to 0/1
      - else => threshold at 0.5
    """
    y = np.array(y)
    if np.any(y == -1):
        y = np.where(y == -1, 0, y)
        y = np.where(y > 0, 1, 0)
    else:
        y = (y >= 0.5).astype(np.int32)
    return y.astype(np.int32)

# -----------------------------
# Weighted BCE (computed on TRAIN)
# -----------------------------
class MultiTaskWeightedBCE(Loss):
    def __init__(self, pos_weight_vec):
        super().__init__()
        self.pos_weight = tf.reshape(tf.cast(pos_weight_vec, tf.float32), (1, -1))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7)

        pos_term = - self.pos_weight * y_true * tf.math.log(y_pred)
        neg_term = - (1. - y_true) * tf.math.log(1. - y_pred)
        return tf.reduce_mean(tf.reduce_sum(pos_term + neg_term, axis=1))

# -----------------------------
# CNN block
# -----------------------------
class CNN(Sequential):
    def __init__(self,
                 num_conv_layers: int,
                 filters: int,
                 kernel_size: int,
                 pool_size: int,
                 dense_units: int,
                 dim_out: int = None,
                 dropout: float = 0.0,
                 l2: float = 1e-4,
                 name: str = "CNN"):
        layers = []
        layers.append(Reshape((-1, 1)))  # (bs, D) -> (bs, D, 1)

        for _ in range(num_conv_layers):
            layers.append(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',
                                 kernel_regularizer=regularizers.l2(l2)))
            layers.append(MaxPooling1D(pool_size=pool_size))
            layers.append(BatchNormalization())

        layers.append(Flatten())

        if dense_units:
            layers.append(Dense(dense_units, activation='relu',
                                kernel_regularizer=regularizers.l2(l2)))

        if dropout > 0.0:
            layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))  # logits

        super().__init__(layers, name=name)

# -----------------------------
# MG-MoE
# -----------------------------
class MultiGateMixtureOfExperts(Model):
    def __init__(self,
                 num_tasks: int,
                 num_emb: int,
                 num_experts: int = 3,

                 num_conv_layers_expert: int = 3,
                 filters_expert: int = 64,
                 kernel_size_expert: int = 3,
                 pool_size_expert: int = 2,
                 dense_units_expert: int = 64,
                 dropout_expert: float = 0.1,
                 l2_expert: float = 1e-4,

                 gate_function: str = "softmax",

                 num_conv_layers_tasks: int = 2,
                 filters_tasks: int = 64,
                 kernel_size_tasks: int = 3,
                 pool_size_tasks: int = 2,
                 dense_units_tasks: int = 64,
                 dim_out_tasks: int = 1,
                 dropout_tasks: float = 0.1,
                 l2_tasks: float = 1e-4):
        super().__init__()

        self.experts = [
            CNN(num_conv_layers_expert, filters_expert, kernel_size_expert, pool_size_expert,
                dense_units_expert, dropout=dropout_expert, l2=l2_expert, name=f"expert_{i}")
            for i in range(num_experts)
        ]

        self.towers = [
            CNN(num_conv_layers_tasks, filters_tasks, kernel_size_tasks, pool_size_tasks,
                dense_units_tasks, dim_out_tasks, dropout=dropout_tasks, l2=l2_tasks, name=f"tower_{t}")
            for t in range(num_tasks)
        ]

        self.gates = [
            Dense(num_experts, activation=gate_function, use_bias=False, name=f"gate_{t}")
            for t in range(num_tasks)
        ]

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        out_experts = []
        expert_input = tf.expand_dims(inputs, -1)  # (bs, D, 1)
        for expert in self.experts:
            out_experts.append(expert(expert_input, training=training))  # (bs, H)
        out_experts = tf.stack(out_experts, axis=-1)  # (bs, H, E)

        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)              # (bs, E)
            mixed = tf.einsum("bhe,be->bh", out_experts, gate_score)  # (bs, H)
            logits = tower(tf.expand_dims(mixed, -1), training=training)  # (bs, 1)
            out_tasks.append(logits)

        out = tf.concat(out_tasks, axis=-1)  # (bs, T) logits
        return tf.nn.sigmoid(out)

# -----------------------------
# ONE split: 70/20/10 (no folds)
# -----------------------------
# First split off TEST = 10%
X_train_val, X_test, y_train_val_raw, y_test_raw = train_test_split(
    X_all, y_raw,
    test_size=0.10,
    random_state=GLOBAL_SEED,
    shuffle=True
)

# Convert to binary
y_train_val = to_binary_labels(y_train_val_raw)
y_test = to_binary_labels(y_test_raw)

# Now split TRAIN vs VAL:
# want VAL = 20% of full => 20/90 = 2/9 of train_val
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val,
    test_size=(2/9),
    random_state=GLOBAL_SEED,
    shuffle=True
)

print(f"Full:  {len(X_all)}")
print(f"Train: {len(X_train)} ({len(X_train)/len(X_all)*100:.2f}%)")
print(f"Val:   {len(X_val)} ({len(X_val)/len(X_all)*100:.2f}%)")
print(f"Test:  {len(X_test)} ({len(X_test)/len(X_all)*100:.2f}%)")

# Compute pos_weight using TRAIN only
eps = 1e-8
pos_counts = np.sum(y_train, axis=0)
neg_counts = y_train.shape[0] - pos_counts
pos_weight = (neg_counts + eps) / (pos_counts + eps)
pos_weight_tf = tf.constant(pos_weight, dtype=tf.float32)

# -----------------------------
# Build + train once
# -----------------------------
epochs = 60
batch_size = 64

tf.keras.backend.clear_session()
tf.random.set_seed(GLOBAL_SEED)
np.random.seed(GLOBAL_SEED)

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=X_train.shape[1],
    num_experts=3,

    num_conv_layers_expert=3, filters_expert=64, kernel_size_expert=3, pool_size_expert=2,
    dense_units_expert=64, dropout_expert=0.1, l2_expert=1e-4,

    gate_function="softmax",

    num_conv_layers_tasks=2, filters_tasks=64, kernel_size_tasks=3, pool_size_tasks=2,
    dense_units_tasks=64, dim_out_tasks=1, dropout_tasks=0.1, l2_tasks=1e-4
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=MultiTaskWeightedBCE(pos_weight_tf),
    metrics=[BinaryAccuracy(name="bin_acc"), AUC(name="auc", multi_label=True)]
)

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_auc", mode="max", patience=7, restore_best_weights=True
)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_auc", mode="max", factor=0.6, patience=3, min_lr=1e-6, verbose=1
)

start = time.process_time()

model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop, reduce_lr],
    verbose=2
)

elapsed_cpu = time.process_time() - start

# -----------------------------
# Tune thresholds on VAL
# -----------------------------
y_val_prob = model.predict(X_val, verbose=0)
# -----------------------------
# Evaluate on TEST
# -----------------------------
y_test_prob = model.predict(X_test, verbose=0)
y_pred_labels = (y_test_prob >= 0.5).astype(int)

f1_micro = f1_score(y_test, y_pred_labels, average='micro', zero_division=0) * 100
f1_macro = f1_score(y_test, y_pred_labels, average='macro', zero_division=0) * 100
print("\n================ FINAL TEST RESULTS ================")
print(f"F1-micro: {f1_micro:.2f}")
print(f"F1-macro: {f1_macro:.2f}")
print(f"CPU time: {elapsed_cpu:.2f} s")

for i, name in enumerate(traits):
    f1_i = f1_score(y_test[:, i], y_pred_labels[:, i], zero_division=0) * 100
    print(f"  Task {name} - F1: {f1_i:.2f}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with MLP (Done)"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential, Model
from keras.layers import Dense, BatchNormalization, Dropout, ReLU, Flatten
from keras.losses import Loss, binary_crossentropy
from keras.metrics import Accuracy, Precision, Recall
from sklearn.metrics import f1_score
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_essays.pkl')
X = np.array(embeddings)
# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=None) -> None:
        super().__init__()

        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        bce = binary_crossentropy(y_true, y_pred)  # (bs, num_tasks)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)  # (1, num_tasks)
        loss = tf.reduce_sum(loss)  # (1)

        return loss


# MLP Model
class MLP(Sequential):
    def __init__(self, num_hidden: int, dim_hidden: int, dim_out: int = None, dropout: float = 0.0, name: str = "MLP"):
        layers = []
        for _ in range(num_hidden - 1):
            layers.append(Dense(dim_hidden))
            layers.append(BatchNormalization())
            layers.append(ReLU())

            if dropout > 0.0:
                layers.append(Dropout(dropout))

        if dim_out:
            layers.append(Dense(dim_out))
        else:
            layers.append(Dense(dim_hidden))

        super().__init__(layers, name=name)

# MGMOE Model
class MultiGateMixtureOfExperts(Model):
    def __init__(self, num_tasks: int, num_emb: int, dim_emb: int = 32, embedding_l2: float = 0.0, num_experts: int = 1,
                 num_hidden_expert: int = 2, dim_hidden_expert: int = 64, dropout_expert: float = 0.0,
                 gate_function: str = "softmax", num_hidden_tasks: int = 2, dim_hidden_tasks: int = 64,
                 dim_out_tasks: int = 1, dropout_tasks: float = 0.0):
        super().__init__()

        # Experts
        self.experts = []
        for _ in range(num_experts):
            self.experts.append(MLP(num_hidden_expert, dim_hidden_expert, dropout=dropout_expert))

        # Towers and Gates for each task
        self.towers = []
        self.gates = []
        for _ in range(num_tasks):
            self.towers.append(MLP(num_hidden_tasks, dim_hidden_tasks, dim_out_tasks, dropout=dropout_tasks))
            self.gates.append(Dense(num_experts, activation=gate_function, use_bias=False))

    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:
        # Experts output
        out_experts = []
        for expert in self.experts:
            out_experts.append(expert(inputs, training=training))  # (bs, num_hidden_expert)

        out_experts = tf.stack(out_experts, axis=-1)  # (bs, num_hidden_expert, num_experts)

        # Task-specific outputs
        out_tasks = []
        for gate, tower in zip(self.gates, self.towers):
            gate_score = gate(inputs, training=training)  # (bs, num_experts)
            in_task = tf.einsum("bie,be->bi", out_experts, gate_score)  # (bs, num_hidden_expert)
            logits_task = tower(in_task, training=training)  # (bs, 1)
            out_tasks.append(logits_task)

        out = tf.concat(out_tasks, -1)  # (bs, num_tasks)
        out = tf.nn.sigmoid(out)  # (bs, num_tasks)

        return out  # (bs, num_tasks)

# Create and compile the model
num_tasks = 5
num_embeddings = X_train.shape[1]  # Assuming that the embeddings are of shape (samples, embedding_size)

model = MultiGateMixtureOfExperts(
    num_tasks=num_tasks,
    num_emb=num_embeddings,
    dim_emb=128,
    num_experts=3,
    num_hidden_expert=2,
    dim_hidden_expert= 64,
    dropout_expert=0.1,
    gate_function="relu",
    num_hidden_tasks=2,
    dim_hidden_tasks=64,
    dim_out_tasks=1,
    dropout_tasks=0.1,
)

loss = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)

model.compile(optimizer=optimizer, loss=loss, metrics=[Accuracy()])

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32)

# Evaluate the model
y_pred = model.predict(X_test)
y_pred_labels = (y_pred > 0.5).astype(int)

# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    # precision = precision_score(y_test[:, i], y_pred_labels[:, i])
    # recall = recall_score(y_test[:, i], y_pred_labels[:, i])
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1*100)

    # print(f"Task {labels_name[i]} - Precision: {precision}, Recall: {recall}, F1-Score: {f1}")
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with Logistic Regression (Done)"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.losses import Loss, binary_crossentropy
from sklearn.metrics import f1_score, precision_score, recall_score
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_essays.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# Loss Function
class MultiTaskBCE(Loss):
    def __init__(self, num_tasks: int, task_weights=None) -> None:
        super().__init__()

        if task_weights is None:
            self.task_weights = tf.ones((1, num_tasks))
        elif tf.rank(task_weights) == 1:
            self.task_weights = tf.expand_dims(task_weights, 0)
        else:
            self.task_weights = task_weights

    def call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        # Reduce the `y_pred` tensor over the hidden dimension to match the shape of `y_true`
        y_pred_reduced = tf.reduce_mean(y_pred, axis=1)  # Reduce along the `num_hidden_expert` dimension

        # Compute the binary crossentropy loss
        bce = binary_crossentropy(y_true, y_pred_reduced)  # Now both `y_true` and `y_pred_reduced` have shape (bs, num_tasks)
        loss = self.task_weights * tf.reduce_mean(bce, axis=0)  # (1, num_tasks)
        loss = tf.reduce_sum(loss)  # (1)
        return loss

# Logistic Regression Model
class LogisticRegression(tf.Module):
    def __init__(self):
        self.built = False

    def __call__(self, x, train=True):
        # Initialize the model parameters on the first call
        if not self.built:
            # Randomly generate the weights and the bias term
            rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)
            rand_b = tf.random.uniform(shape=[], seed=22)
            self.w = tf.Variable(rand_w)
            self.b = tf.Variable(rand_b)
            self.built = True

        # Compute the model output
        z = tf.add(tf.matmul(x, self.w), self.b)
        z = tf.squeeze(z, axis=1)

        if train:
            return z
        return tf.sigmoid(z)

# MGMOE Model with Logistic Regression Experts
class MultiGateMixtureOfExperts(tf.Module):
    def __init__(self, num_tasks: int, num_experts: int = 1, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts and gates for each task
        self.experts = [LogisticRegression() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def __call__(self, inputs: tf.Tensor, train=True):
        # Get experts' outputs
        out_experts = tf.stack([expert(inputs, train=train) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Expand dimensions of gate_scores to align with out_experts
            gate_scores = tf.expand_dims(gate_scores, axis=1)  # (batch_size, 1, num_experts)

            # Element-wise multiplication and summation along the expert dimension
            task_output = tf.reduce_sum(out_experts * gate_scores, axis=-1)  # (batch_size, num_hidden_expert)
            out_tasks.append(task_output)

        # Stack the outputs and apply sigmoid for final probabilities
        out = tf.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_hidden_expert, num_tasks)
        if not train:
            out = tf.sigmoid(out)

        return out

# Instantiate and compile the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

loss_fn = MultiTaskBCE(num_tasks=num_tasks)
optimizer = tf.keras.optimizers.Adam()

# Training loop
def train_step(model, inputs, labels):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = loss_fn(labels, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

# Training the model
epochs = 30
batch_size = 32
num_batches = X_train.shape[0] // batch_size

for epoch in range(epochs):
    epoch_loss = 0
    for i in range(num_batches):
        batch_x = X_train[i*batch_size:(i+1)*batch_size]
        batch_y = y_train[i*batch_size:(i+1)*batch_size]
        batch_loss = train_step(model, batch_x, batch_y)
        epoch_loss += batch_loss
    print(f"Epoch {epoch+1}, Loss: {epoch_loss.numpy()}")

# Evaluate the model
y_pred = model(X_test, train=False)
y_pred = tf.reduce_mean(y_pred, axis=1)  # Ensure y_pred has the shape (batch_size, num_tasks)
y_pred_labels = (y_pred.numpy() > 0.5).astype(int)  # Convert to binary labels

# Make sure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)

# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with Random Forest Models (Done)"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_essays.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# RandomForest Experts
class RandomForestExpert:
    def __init__(self, num_trees=500, max_depth=5, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0.001, max_features='sqrt', ccp_alpha=0.01):
        # Create a random forest classifier for each expert with optimized hyperparameters
        self.model = RandomForestClassifier(
            n_estimators=num_trees,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=42,
            n_jobs=-1,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MMOE Model with Random Forest Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (RandomForest models) and gates for each task
        self.experts = [RandomForestExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)


# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")
# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with Support Vector Classifiers (SVC) (Done)"""

import numpy as np
import time
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_score, recall_score
import tensorflow as tf
import time
start = time.perf_counter()      # high-res timer, returns seconds as float
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('bert_embeddings_essays.pkl')
X = np.array(embeddings)
# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)


start = time.process_time()
# Support Vector Classifier (SVC) Expert Class
class SVCExpert:
    def __init__(self, C=200, kernel='linear', degree=3, gamma='scale', tol=0.0001, class_weight=None, random_state=42, max_iter=-1):
        # Create a support vector classifier for each expert with optimized hyperparameters
        self.model = SVC(
            C=C,
            kernel=kernel,
            degree=degree,
            gamma=gamma,
            tol=tol,
            class_weight=class_weight,
            random_state=random_state,
            max_iter=max_iter,
            probability=True  # Enable probability estimates
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # Support Vector Classifier provides probability estimates if probability=True
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with SVC Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (SVC models) and gates for each task
        self.experts = [SVCExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)

# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")
# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")
elapsed_cpu = time.process_time() - start
print(f"CPU time: {elapsed_cpu:.6f} s")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with  Decision Trees Models (Done)"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
import tensorflow as tf
import time
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]
# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_essays.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)
start = time.process_time()
# Decision Tree Expert Class with optimized parameters
class DecisionTreeExpert:
    def __init__(self, criterion='log_loss', splitter='best', max_depth=10, min_samples_split=2, min_samples_leaf=1,
                 min_weight_fraction_leaf=0.0, max_features=None, random_state=42, max_leaf_nodes=None,
                 min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.1):
        # Create a decision tree classifier for each expert with optimized hyperparameters
        self.model = DecisionTreeClassifier(
            criterion=criterion,
            splitter=splitter,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            min_weight_fraction_leaf=min_weight_fraction_leaf,
            max_features=max_features,
            random_state=random_state,
            max_leaf_nodes=max_leaf_nodes,
            min_impurity_decrease=min_impurity_decrease,
            class_weight=class_weight,
            ccp_alpha=ccp_alpha
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # DecisionTree supports predict_proba if it's a classification task
        if hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class
        else:
            return self.model.predict(X)

# MGMOE Model with Decision Tree Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (DecisionTree models) and gates for each task
        self.experts = [DecisionTreeExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3

model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")

# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)

# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)

# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)

# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")

elapsed_cpu = time.process_time() - start
print(f"CPU time: {elapsed_cpu:.6f} s")

"""#Multi-task Learning using Mixture of Experts (MGMOE) with XGBClassifier (XGB) (Done)



"""

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from xgboost import XGBClassifier
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
# Load and preprocess the data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)
print("Data shape:", data.shape)

# Extract text data and labels
texts = data['text']
labels = data[['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']]

# Load the saved BERT embeddings
embeddings = joblib.load('/content/bert_embeddings_essays.pkl')
X = np.array(embeddings)

# Define labels for each task
y1 = labels['cEXT1']
y2 = labels['cNEU1']
y3 = labels['cAGR1']
y4 = labels['cCON1']
y5 = labels['cOPN1']

# Combine labels into a single array for multi-task learning
y = np.column_stack((y1, y2, y3, y4, y5))

# Replace -1 labels with 0 for binary classification (or map classes appropriately)
y = np.where(y == -1, 0, y)

# Split data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# XGBClassifier Expert Class
class XGBExpert:
    def __init__(self):
        # Create an XGBClassifier with optimized hyperparameters
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=2,
            learning_rate=0.001,
            objective='binary:logistic',
            eval_metric='mlogloss',
            random_state=42
        )

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def predict_proba(self, X):
        # XGBClassifier provides probability estimates
        return self.model.predict_proba(X)[:, 1]  # Return the probability of the positive class

# MGMOE Model with XGB Experts
class MultiGateMixtureOfExperts:
    def __init__(self, num_tasks: int, num_experts: int = 3, gate_function: str = "softmax"):
        self.num_tasks = num_tasks
        self.num_experts = num_experts

        # Initialize experts (XGB models) and gates for each task
        self.experts = [XGBExpert() for _ in range(num_experts)]
        self.gates = [tf.Variable(tf.random.uniform([X_train.shape[1], num_experts], seed=42), trainable=True) for _ in range(num_tasks)]

    def train_experts(self, X_train, y_train):
        # Train each expert on the training data
        for i in range(self.num_experts):
            self.experts[i].train(X_train, y_train[:, i])

    def __call__(self, inputs: tf.Tensor):
        # Get experts' outputs (logits)
        out_experts = np.stack([expert.predict_proba(inputs) for expert in self.experts], axis=-1)

        # Get task-specific outputs
        out_tasks = []
        for gate in self.gates:
            gate_scores = tf.nn.softmax(tf.matmul(inputs, gate), axis=-1)  # Apply softmax to gates

            # Element-wise multiplication and summation along the expert dimension
            task_output = np.sum(out_experts * gate_scores.numpy(), axis=-1)  # (batch_size,)
            out_tasks.append(task_output)

        # Stack the outputs
        out = np.stack(out_tasks, axis=-1)  # Shape: (batch_size, num_tasks)

        return out

# Instantiate and train the model
num_tasks = 5
num_experts = 3
model = MultiGateMixtureOfExperts(num_tasks=num_tasks, num_experts=num_experts, gate_function="softmax")
# Train experts on each task
model.train_experts(X_train, y_train)

# Predict on the test set
y_pred = model(X_test)
# Convert probabilities to binary labels
y_pred_labels = (y_pred > 0.5).astype(int)
# Ensure y_test and y_pred_labels are both in binary multilabel format
assert y_test.shape == y_pred_labels.shape  # Ensure both have the shape (batch_size, num_tasks)
# Calculate global TP, FP, FN for F1-micro calculation across all tasks
tp, fp, fn = 0, 0, 0
num_tasks = 5
for i in range(num_tasks):
    tp += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 1))
    fp += np.sum((y_test[:, i] == 0) & (y_pred_labels[:, i] == 1))
    fn += np.sum((y_test[:, i] == 1) & (y_pred_labels[:, i] == 0))

# Calculate precision, recall, and F1-micro manually
precision_micro_manual = tp / (tp + fp) if (tp + fp) > 0 else 0
recall_micro_manual = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_micro_manual = 2 * (precision_micro_manual * recall_micro_manual) / (precision_micro_manual + recall_micro_manual) if (precision_micro_manual + recall_micro_manual) > 0 else 0

print(f"Manually calculated F1-micro: {f1_micro_manual}")

# Calculate F1-macro by averaging per-task F1 scores
f1_macro_manual = np.mean([f1_score(y_test[:, i], y_pred_labels[:, i]) for i in range(num_tasks)])

print(f"Manually calculated F1-macro: {f1_macro_manual}")

# Calculate F1-micro and F1-macro using sklearn for comparison
f1_micro_sklearn = f1_score(y_test, y_pred_labels, average='micro')
f1_macro_sklearn = f1_score(y_test, y_pred_labels, average='macro')

print(f"F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (sklearn): {f1_macro_sklearn}")

# Print performance metrics for each task
labels_name = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
F1_score = []
for i in range(num_tasks):
    f1 = f1_score(y_test[:, i], y_pred_labels[:, i])
    F1_score.append(f1 * 100)
    print(f"Task {labels_name[i]} - F1-Score: {f1}")

# Print comparison
print(f"F1-micro (manual): {f1_micro_manual} vs F1-micro (sklearn): {f1_micro_sklearn}")
print(f"F1-macro (manual): {f1_macro_manual} vs F1-macro (sklearn): {f1_macro_sklearn}")