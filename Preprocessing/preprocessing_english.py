# -*- coding: utf-8 -*-
"""Preprocessing_English.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17yR4vDL19YwrlvwQPwjDtXRyuGu6xNMo

#Libraries
"""

# Data Analysis
import pandas as pd
from pandas import read_csv
import numpy as np
from numpy import asarray
from numpy import savetxt
from numpy import loadtxt
import pickle as pkl
from scipy import sparse

# Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt
import wordcloud
from wordcloud import WordCloud, STOPWORDS

# Text Processing
import re
import itertools
import string
import collections
from collections import Counter
from sklearn.preprocessing import LabelEncoder
import nltk
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Machine Learning packages
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import sklearn.cluster as cluster
from sklearn.manifold import TSNE

# Model training and evaluation
from sklearn.model_selection import train_test_split

#Models
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from xgboost import plot_importance

#Metrics
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix
from sklearn.metrics import classification_report

# The python natural language toolkit library provides a list of english stop words.
import nltk
nltk.download('stopwords')
#print(stopwords.words('english'))

#over_sampling multi label
from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import RandomOverSampler
#SMOTE Over Sampling
from imblearn.over_sampling import SMOTE
from collections import Counter
from matplotlib import pyplot
import csv

# Ignore noise warning
import warnings
warnings.filterwarnings("ignore")

"""#Essays Dataset"""

df_essays = pd.read_csv('/content/updated_dataset (1).csv')
# df_essays = pd.read_csv('/content/drive/MyDrive/filespython/input/essays.csv')
print(df_essays.shape)
df_essays.head()

print('Length : ', len(df_essays['text'][1584].split(' ')))
print(len(df_essays['text'][1584]))
df_essays['TEXT'][1584]

from gensim.parsing.preprocessing import remove_stopwords
import re
import nltk
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer
nltk.download("wordnet")
nltk.download("omw-1.4")
def preprocess_text(df, remove_special=True):
    texts = df['TEXT'].copy()

    # #Remove stop words
    # df["posts"] = df["posts"].apply(lambda x: remove_stopwords(x))

    #Remove links
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'https?:\/\/.*?[\s+]', ' ', x.replace("|"," ") + " "))

    #Keep the End Of Sentence characters
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'\.', '', x + " "))
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'\?', '', x + " "))
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'!', '', x + " "))

    #Strip Punctation
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'[\.+]', ".",x))

    #Remove multiple fullstops
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'[^\w\s]',' ',x))

    # #Remove Non-words
    # df["posts"] = df["posts"].apply(lambda x: re.sub(r'[^a-zA-Z\s]',' ',x))

    #Convert posts to lowercase
    df["TEXT"] = df["TEXT"].apply(lambda x: x.lower())

    # #Remove multiple letter repeating words
    # df["posts"] = df["posts"].apply(lambda x: re.sub(r'([a-z])\1{2,}[\s|\w]*',' ',x))

    #Remove very short or long words
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'(\b\w{0,2})?\b',' ',x))
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r'(\b\w{30,1000})?\b',' ',x))

    #Remove extra Spaces to one space
    #result = " ".join(train[1].split())
    df["TEXT"] = df["TEXT"].apply(lambda x: re.sub(r' +',' ',x))

    Number = [' zero ',' one ', ' two ', ' three ', ' four ', ' five ', ' six ', ' seven ', ' eight ', ' nine ', ' ten ', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']
    for i in Number:
      df['TEXT'] = df['TEXT'].apply(lambda x: re.sub(i,'',x))

    df["TEXT"] = df["TEXT"].apply(lambda x: remove_stopwords(x))

    list_stops = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "youre", "youve", "youll", "youd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "shes", 'her', 'hers', 'herself', 'it', "its", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "thatll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "dont", 'should', "shouldve", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "arent", 'couldn', "couldnt", 'didn', "didnt", 'doesn', "doesnt", 'hadn', "hadnt", 'hasn', "hasnt", 'haven', "havent", 'isn', "isnt", 'ma', 'mightn', "mightnt", 'mustn', "mustnt", 'needn', "neednt", 'shan', "shant", 'shouldn', "shouldnt", 'wasn', "wasnt", 'weren', "werent", 'won', "wont", 'wouldn', "wouldnt"]
    #print(len(new_df['posts'][1]))
    for i in range(len(df)):
      a1 = df['TEXT'][i]
      for j in range(len(list_stops)):
        a1 = a1.replace(" "+list_stops[j]+" " , ' ')
        #p = re.compile('(\s*)'+list_stops[j]+'(\s*)')
        #df['posts'][i] = p.sub(' ', a1)
      df['TEXT'][i] = a1

    # ST_W = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "youre", "youve", "youll", "youd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "shes", 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "dont", 'should', "shouldve", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "arent", 'couldn', "couldnt", 'didn', "didnt", 'doesn', "doesnt", 'hadn', "hadnt", 'hasn', "hasnt", 'haven', "havent", 'isn', "isnt", 'ma', 'mightn', "mightnt", 'mustn', "mustnt", 'needn', "neednt", 'shan', "shant", 'shouldn', "shouldnt", 'wasn', "wasnt", 'weren', "werent", 'won', "wont", 'wouldn', "wouldnt"]
    # for i in ST_W:
    #   df['posts'] = df['posts'].apply(lambda x: re.sub(i,'',x))

    #example_sentence = "finding lack posts alarming boring position example girlfriend currently environment creatively cowgirl missionary giving meaning game theory hello grin thats takes converse flirting acknowledge presence return words smooth wordplay cheeky grins lack balance hand coordination real test score internet tests funny score higher like responses thread mention believe test banish know vanish site year half return people commenting posts liking ideasthoughts know think things sherlock holmes quote special knowledge special powers like encourages seek complex cheshirewolftumblrcom post thought real functions judge dominates emotions rarely strength know ingenious saying want happens playing person shooter drive want look rock paper best makes guys lucky high tumblr hear person shooter game rocking hell soundtrack auto sound equipment shake heavens managed couple connected things dominates aware environments dominates example shawn spencer patrick jane charlie admit jealous like chalk heart mixed dominate like noticed like known upload clip away mouth hear ninja assassin style splatter great song long mental block singer love beat makes bounce dropio vswck close mouth smokin aces assassins ball playing background sociable extrovert extrovert sociable sherlock movie normally played extj books said movie looked good called sherlock holmes fear kissing kiss animal vanish personal taste liking kissed know sounds pretty like area going right trying figure want life want things biggest problem know operating impression female looked boxy okay help friends time developed little crush described living worst nightmare trapped place dull woods serial killer perfect place sadly biased sounds like shadowed think maybe hurt turned tell typical traits left checks list sorry came time weve reached quota female like females deal kick antp leaning easy s identify imagine interrogations little like jacks mechanical rigging shock treatment equipment abandoned building batty jumper compliment trust psychopathic emoticons theyre weird ones like laughing hurt people running lawn mower like theme live know heart usual leave thing ends mean time times work thing work pleasure meet damn need trust instincts closer going exfp leaning responded friends lesbian ones come advice masters great able build building duck duck duck shotgun hard losing like knew right awesome correct tell stupid know play makes laugh going neuropsychology psychologist nightowl wake stay awake till personal opinion backed theory suggest socially difficult socially indifferent social situations need arises personal stocks desktop downloaded random stock sites stock photobuckets tell open photoshop glad like static thanks friend hours work constructed line static avatar later fellow teammates psychologist long diagnosis like diagnosis psychologist friends friends tell"
    #example_sentence = "Python programmers went often taken tend liked programming in python because it's like english. We call people who program in python pythonistas."
    wnl = WordNetLemmatizer()
    for i in range(len(df)):
      example_sentence = df['TEXT'][i]
      K = ""
      # Remove punctuation
      example_sentence_no_punct = example_sentence.translate(str.maketrans("", "", string.punctuation))
      word_tokens = word_tokenize(example_sentence_no_punct)
      # Perform lemmatization
      #print("{0:20}{1:20}".format("--Word--","--Lemma--"))
      for word in word_tokens:
        #print ("{0:20}{1:20}".format(word, wnl.lemmatize(word, pos="v")))
        K = K + wnl.lemmatize(word, pos="v") + " "
      df['TEXT'][i] = K
      K =""

    return df

#Preprocessing of entered Text
new_df = preprocess_text(df_essays)

#Remove posts with less than X words
min_words = 0
print("Before : Number of posts", len(new_df))
new_df["no. of. words"] = new_df["TEXT"].apply(lambda x: len(re.findall(r'\w+', x)))
new_df = new_df[new_df["no. of. words"] >= min_words]

print("After : Number of posts", len(new_df))
new_df.head()

# Drop the desired column(s)
columns_to_drop = ['#AUTHID', 'Unnamed: 7']
new_df = new_df.drop(columns=columns_to_drop)
new_df

# Check for null values
null_values = new_df.isnull().sum()

# Display the null values count for each column
print(null_values)

# Create a new column with 1 for 'yes' and 0 for 'no'
new_df['cEXT1'] = new_df['cEXT'].map({'y': 1, 'n': 0}).fillna(-1).astype(int)
new_df['cNEU1'] = new_df['cNEU'].map({'y': 1, 'n': 0}).astype(int)
new_df['cAGR1'] = new_df['cAGR'].map({'y': 1, 'n': 0}).astype(int)
new_df['cCON1'] = new_df['cCON'].map({'y': 1, 'n': 0}).astype(int)
new_df['cOPN1'] = new_df['cOPN'].map({'y': 1, 'n': 0}).astype(int)
new_df

new_df

#Save File Dataset after preprocessing
new_df.to_csv('updated_dataset_preprocessing (1).csv', encoding='utf-8')