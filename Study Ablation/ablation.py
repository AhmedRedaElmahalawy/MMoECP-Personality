# -*- coding: utf-8 -*-
"""Ablation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGRyQ_cTGEHTXXNU9QRRNyUXSf_1uitP

CNN
"""

import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from keras import Model
from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dense, Dropout, Flatten, Reshape
from keras.losses import BinaryCrossentropy

# Reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)

traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
y = data[traits].values.astype(int)

X = np.array(joblib.load('/content/bert_embeddings_essays.pkl'), dtype=np.float32)

# Split (70/20/10)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# CNN w/o MMoE
class MultiTaskCNN(Model):
    def __init__(self, num_tasks):
        super().__init__()
        self.reshape = Reshape((-1, 1))
        self.conv1 = Conv1D(64, 3, activation='relu')
        self.pool1 = MaxPooling1D(2)
        self.bn1 = BatchNormalization()
        self.conv2 = Conv1D(64, 3, activation='relu')
        self.pool2 = MaxPooling1D(2)
        self.bn2 = BatchNormalization()
        self.flatten = Flatten()
        self.fc = Dense(64, activation='relu')
        self.dropout = Dropout(0.1)
        self.out = Dense(num_tasks, activation='sigmoid')

    def call(self, x):
        x = self.reshape(x)
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.bn1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.bn2(x)
        x = self.flatten(x)
        x = self.fc(x)
        x = self.dropout(x)
        return self.out(x)

model = MultiTaskCNN(num_tasks=5)
model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss=BinaryCrossentropy(),
)

model.fit(X_train, y_train, validation_data=(X_val, y_val),
          epochs=30, batch_size=32, verbose=2)

# Evaluation (threshold = 0.5)
y_pred = (model.predict(X_test) >= 0.5).astype(int)

f1_micro = f1_score(y_test, y_pred, average='micro')
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"CNN w/o MMoE | F1-micro: {f1_micro:.4f} | F1-macro: {f1_macro:.4f}")

"""MLP"""

import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from keras import Model
from keras.layers import Dense, BatchNormalization, Dropout
from keras.losses import BinaryCrossentropy

# Reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load data
data = pd.read_csv('/content/essays_preprocessing.csv')
data.dropna(inplace=True)

traits = ['cEXT1', 'cNEU1', 'cAGR1', 'cCON1', 'cOPN1']
y = data[traits].values.astype(int)

X = np.array(joblib.load('/content/bert_embeddings_essays.pkl'), dtype=np.float32)

# Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)

# MLP w/o MMoE
class MultiTaskMLP(Model):
    def __init__(self, num_tasks):
        super().__init__()
        self.fc1 = Dense(128)
        self.bn1 = BatchNormalization()
        self.fc2 = Dense(64)
        self.bn2 = BatchNormalization()
        self.dropout = Dropout(0.1)
        self.out = Dense(num_tasks, activation='sigmoid')

    def call(self, x):
        x = tf.nn.relu(self.bn1(self.fc1(x)))
        x = tf.nn.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        return self.out(x)

model = MultiTaskMLP(num_tasks=5)
model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-4),
    loss=BinaryCrossentropy(),
)

model.fit(X_train, y_train, validation_data=(X_val, y_val),
          epochs=30, batch_size=32, verbose=2)
y_pred = (model.predict(X_test) >= 0.5).astype(int)

f1_micro = f1_score(y_test, y_pred, average='micro')
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"MLP w/o MMoE | F1-micro: {f1_micro:.4f} | F1-macro: {f1_macro:.4f}")